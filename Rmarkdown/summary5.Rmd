---
title: "summary5"
author: "Jian Cao"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

In this markdown, I would like to check the derivative w.r.t. the squared relevance parameter at zero. Specifically,

* whether the derivatives are also zero
* whether we can use the squared relevance parameter to achieve variable selection
    + whether penalty is needed
    + the computation efficiency

Answers:

* No, parameters **can return to non-zero from zero values**
* Seems not as efficient as using relevance parameters
    + without penalty, fake relevance parameters converge to zero slower
    + if we don't ignore the zero-valued parameters, the computation cost will be much higher

## Preparation
Need to install a modified version of the GpGp R package.
```{r install the moderated GpGp Pkg}
devtools::install_github("https://github.com/SamCao1991/GpGp.git")
library(GpGp)
source("quad_cdsc_L1.R")
```

## Simulate GP in $\mathbb{R}^{d}$
```{r simulate GP in d dimensions}
set.seed(123)
n <- 1e4
d <- 20
rSq <- c(10, 5, 2, 1, 0.5, rep(0, d - 5))^2
sigmasq <- 1.0 # variance
tausq <- 0.05^2 # nugget
locs <- lhs::randomLHS(n, d)
locs <- locs * outer(rep(sqrt(n), n), 1 / sqrt(colSums(locs^2)))
covM <- GpGp::matern25_scaledim_sqrelevance(c(sigmasq, rSq, tausq), locs)
cholM <- t(chol(covM))
y <- as.vector(cholM %*% rnorm(n))
X <- matrix(1, n, 1)
rm(covM, cholM)
```
Notice that we still have only 5 non-zero relevance parameters, the first five. Define the conditioning size `m`.
```{r conditioning size}
m <- 100
```

## Optimization 
Define initial values for all parameters. Notice that here I use the squared relevance parameters.
```{r init val for non-rel parms-penalty parms-opt bounds}
sigmasqInit <- 0.25
tausqInit <- 0.01^2
rSqInit <- rep(1, d)
lambda <- 0
lb_nonrel_parms <- c(0.01^2, 0.01^2) 

```
`lb_nonrel_parms` defines the lower limit of $\sigma^2$ and $\tau^2$ during the optimization using `quad_cdsc_L1`.

Fit with `quad_cdsc_L1`. To make it more interesting, we set the initial values of the true relevance parameters to zero and fake relevance parameters to one.
```{r}
startTime <- Sys.time()
crtIter <- 1
maxIter <- 100
theta <- c(sigmasqInit, c(rep(0, 5), rep(1, d - 5)), tausqInit)
while(maxIter >= crtIter)
{
  locsScal <- locs %*% diag(sqrt(theta[2 : (d + 1)]))
  odr <- GpGp::order_maxmin(locsScal)
  yOdr <- y[odr]
  locsScalOdr <- locsScal[odr, , drop = F]
  locsOdr <- locs[odr, , drop = F]
  XOdr <- X[odr, , drop = F]
  NNarray <- GpGp::find_ordered_nn(locsScalOdr, m = m)
  # Define functions for parameter estimation in the outer loop
  objfun <- function(theta, locsOdr){
    GpGp::vecchia_profbeta_loglik(theta, "matern25_scaledim_sqrelevance",
                                            yOdr, XOdr, locsOdr, 
                                            NNarray)
  }
  objfun_gdfm <- function(theta, locsOdr){
    GpGp::vecchia_profbeta_loglik_grad_info(theta, "matern25_scaledim_sqrelevance",
                                            yOdr, XOdr, locsOdr, 
                                            NNarray)
  }
  
  theta <- quad_cdsc_L1_brute(objfun, objfun_gdfm, locsOdr, 1, theta, lambda, 1e-3, silent = T, 
               max_iter = min(crtIter, maxIter - crtIter + 1), max_iter2 = 40, 
               lb_nonrel_parms = lb_nonrel_parms)$covparms
  cat("quad_cdsc_L1 fit iter", crtIter, ": estimated parms = ", theta, "\n")
  crtIter <- crtIter + min(crtIter, maxIter - crtIter + 1)
}
endTime <- Sys.time()
cat("Final estimates are", theta, "\n")
cat("Time used for final estimation is", as.numeric(difftime(endTime, startTime, units = "secs")), "seconds\n")
```

Fit with `fisher_scoring`. For `fisher_scoring`, I set all the initial values for all relevance parameters to one.
```{r}
startTime <- Sys.time()
crtIter <- 1
maxIter <- 100
theta <- c(sigmasqInit, rSqInit, tausqInit)
linkfuns <- GpGp::get_linkfun("matern25_scaledim")
link <- linkfuns$link
dlink <- linkfuns$dlink
invlink <- linkfuns$invlink
while(maxIter >= crtIter)
{
  locsScal <- locs %*% diag(sqrt(theta[2 : (d + 1)]))
  odr <- GpGp::order_maxmin(locsScal)
  yOdr <- y[odr]
  locsScalOdr <- locsScal[odr, , drop = F]
  locsOdr <- locs[odr, , drop = F]
  XOdr <- X[odr, , drop = F]
  NNarray <- GpGp::find_ordered_nn(locsScalOdr, m = m)
  # Define functions for parameter estimation in the outer loop
  objfun <- function(thetaTrans){
  likobj <- 
    GpGp::vecchia_profbeta_loglik_grad_info(link(thetaTrans), 
                                            "matern25_scaledim_sqrelevance",
                                            yOdr, XOdr, locsOdr, 
                                            NNarray)
    likobj$loglik <- -likobj$loglik
    likobj$grad <- -c(likobj$grad)*dlink(thetaTrans)
    likobj$info <- likobj$info*outer(dlink(thetaTrans),dlink(thetaTrans)) 
    return(likobj)
  }
  
  thetaTrans <- invlink(theta)
  theta <- link(fisher_scoring(objfun, thetaTrans, link, T, 1e-3,
                      min(crtIter, maxIter - crtIter + 1))$logparms)
  cat("fisher_scoring fit iter", crtIter, ": estimated parms = ", theta, "\n")
  crtIter <- crtIter + min(crtIter, maxIter - crtIter + 1)
}
endTime <- Sys.time()
cat("Final estimates are", theta, "\n")
cat("Time used for final estimation is", as.numeric(difftime(endTime, startTime, units = "secs")), "seconds\n")
```













