---
title: "Matthias_comments"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

>  If I understand correctly, your 4th bullet is similar to what I had in mind. When d is very large, it is not feasible to start with small or zero lambda and initialize all relevances as nonzero. Instead, I think we need to start with a large lambda and most relevances equal to zero, and hopefully be able to optimize while exploiting sparsity. Then we could start decreasing lambda gradually to allow less sparse solutions, but (again, hopefully) stop this procedure before we get to a small lambda and very dense solution (e.g., because the cross validation indicates that the solutions are getting worse).

When using the relevance parameters, I agree that we need to start with most relevances equal to zero. In this case, we can exploit the sparsity quite well, i.e., computing the gradient and FIM w.r.t. non-zero relevance parameters. 

As for decreasing lambda to allow less sparsity, I am not sure if this can be done when using the relevance parameters since zero-valued parameters will stay at zero. However, in terms of variable selection, I am considering the following procedure:

* For each $\lambda$ in the pool
    + For each training set, initialize $\boldsymbol{\theta} = \mathbf{0}$
        + $\mbox{sz} = 20$, for $i \in 1 : \frac{d}{\mbox{sz}}$
            + $\tau = \mbox{seq(from = i, to = d, by = d / sz)}$  
            $\boldsymbol{\theta}_{\tau} = \mathbf{1}$, 
            optimize$(\boldsymbol{\theta})$ under $\lambda$ for a small number of iterations
        + optimize$(\boldsymbol{\theta})$ under $\lambda$ for some more iterations
        + Compute the MSE with the testing set
    + Average the MSE across different training/testing sets
* Choose the best $\lambda$
* Use the entire dataset to repeat the training process above

> Related to that, you said on Overleaf that "relevance parameters that already reach zero will stay at zero." As I wrote in a previous email:
"What if we did the optimization wrt the square root of the relevances, say $s_l = (r_l)^{1/2}$? Would the derivative wrt to $s_l$ at $s_l=0$ be nonzero? Would that work?
If not, do you have any other suggestions for how to jump-start? This will be crucial when we do the optimization sequentially for different penalty parameters, starting with the empty model."

I think that $s_l = (r_l)^{1/2} \Rightarrow r_l = s_l^2$.
$$
  \frac{\partial l}{\partial s_l} = \frac{\partial l}{\partial r_l} \frac{\partial r_l}{\partial s_l} = 0 \times 2 s_l = 0
$$
If we consider $s_l = (r_l)^{2}$, assuming $d_{ij}$ is the distance between the $i$-th and $j$-th locations under the `scaled' location coordinates.
\begin{align*}
  \frac{\partial \boldsymbol{\Sigma_{ij}}}{\partial s_l} &= \frac{\partial \boldsymbol{\Sigma_{ij}}}{\partial d_{ij}} \frac{\partial d_{ij}}{\partial r_l} \frac{\partial r_l}{\partial s_l} \\
  &= \frac{\partial \boldsymbol{\Sigma_{ij}}}{\partial d_{ij}} \frac{c_1 \cdot r_l}{d_{ij}} \frac{1}{2} \frac{1}{r_l} \\
  &= c_2 \cdot \frac{\partial \boldsymbol{\Sigma_{ij}}}{\partial d_{ij}} \frac{1}{d_{ij}}
\end{align*} 
Hence, $\frac{\partial \boldsymbol{\Sigma_{ij}}}{\partial s_l}$ should be non-zero. However, it requires further experiment to see if they can return to non-zero values from zero.

As for jump-start, my idea is basically the pseudo-algorithm from the previous answer. I think the previous jump-start algorithm is more efficient since we only need to compute the gradient and the FIM for a small number of parameters but not sure which one is more effective in terms of variable selection.

> Also related to 1), I'm still not sure how you know how to pre-determine reasonable lambda values. For example, how large would lambda have to be to get a completely sparse solution? How did you come up with the value of 10 in your example? As I wrote in a previous email:
"What exact range of lambda values are you proposing? I guess the idea would be to have a lambda grid whose upper endpoint is large enough (i.e., strong enough penalization) to obtain a completely empty model, and whose lower endpoint is small enough so that all covariates are included. Is that guaranteed with your suggestion?"

The choice of $\lambda$ I am proposing is $0, 1, 10, \ldots, n$. Thus, the smaller end is without penalty but not sure whether all covariates will be included. In fact, I think that the sparsity in the relevance parameters still exists when $\lambda = 0$ because it feels that the relevance parameters for insignificant dimensions tend to become smaller or just reach zero. As for the bigger end of $\lambda$, empirically before $\lambda$ reaches $n$, all relevance parameters will be optimized to zero.

Upon reflecting your comments, I have another idea similar to binary search:

* Start with $\lambda = n$
* While (#non-zero parms < $d_0$ and $\lambda$ > $\lambda_0$)
    + Optimize $\boldsymbol{\theta}$ using $\lambda$
    + Report (#non-zero parms, $\boldsymbol{\theta}$)
    + $\lambda = \lambda / 2$

If we include cross-validation, the pseudo-algorithm would be:

* Start with $\lambda = n$
* While (#non-zero parms < $d_0$ and $\lambda$ > $\lambda_0$)
    + Cross-validation using $\lambda$
    + Fit whole dataset with $\lambda$
    + Report (#non-zero parms, $\boldsymbol{\theta}$, loss)
    + $\lambda = \lambda / 2$
    
What do you think of the two methods of choosing $\lambda$?

> By the way, in your code it looks like you are doing the maximin ordering in the scaled input space, but the nearest-neighbor search in the original input space. I recommend doing both in the scaled space. That is what we did in SVecchia (https://github.com/katzfuss-group/scaledVecchia/blob/master/vecchia_scaled.R).

Yep, thanks! I need to change the code to base the nearest-neighbor searching on the ordered scaled locations.

> In addition, you may want to try a larger m, to see if that leads to improved solutions. $m = 30$ is reasonably large in 2 input dimensions, but it is probably too small to get good approximation accuracy in 20 or even 100 dimensions.

Sure, I will try $m = 50$ and $m = 100$ in the next simulation.

> This is not a high priority right now, but eventually it would be interesting to explore what happens when the covariates are correlated (as will likely often be the case in "real" regression settings) and to compare the methods in terms of prediction scores (e.g., RMSE and log score).

OK, so let's consider this later. To generate more correlated columns in `locs`, I guess that it means not using the hyper Latin cude generating method.

> Regarding Joe's suggestion of a cutoff for Fisher scoring, we provided that option (by setting the function argument "select" to a finite value) in SVecchia.

Yes, good to know ) Although the implementation is outside the Fisher_scoring algorithm, the computation savings and the effectiveness of variable selection should be similar. However, two questions may need consideration:

* How to choose the threshold $r_0$
* This means that once a relevance parameter reaches $r_0$, it will stay at zero, which is stricter than that once a relevance parameter reaches $0$, it will stay at zero






