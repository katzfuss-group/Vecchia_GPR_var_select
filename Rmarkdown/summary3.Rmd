---
title: "summary3"
author: "Jian Cao"
date: "February 1, 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

In this markdown, I would like to show an effective and efficient way for variable selection when $d$ is big, e.g., $1000$. As mentioned in **summary2**, this method is `quad_cdsc_L1` combined with

* with starting with a small subset of all relevance parameters being non-zero 
* cross validation 

## Preparation
Need to install a modified version of the GpGp R package.
```{r}
devtools::install_github("https://github.com/SamCao1991/GpGp.git")
library(GpGp)
```
Need to load the `quad_cdsc_L1`, the `batch_start`, and the `cross_valid` functions.
```{r}
source("quad_cdsc_L1.R")
source("cross_valid.R")
source("batch_start.R")
```
## Simulate GP in $\mathbb{R}^{1000}$
```{r}
set.seed(123)
n <- 1e4
d <- 1000
r <- c(10, 5, 2, 1, 0.5, rep(0, d - 5))
sigmasq <- 1.0 # variance
tausq <- 0.05^2 # nugget
locs <- lhs::randomLHS(n, d)
locs <- locs * outer(rep(sqrt(n), n), 1 / sqrt(colSums(locs^2)))
covM <- GpGp::matern25_scaledim_relevance(c(sigmasq, r, tausq), locs)
cholM <- t(chol(covM))
y <- as.vector(cholM %*% rnorm(n))
X <- matrix(1, n, 1)
rm(covM, cholM)
```
Notice that we still have only 5 non-zero relevance parameters, the first five. Define the conditioning size `m`.
```{r}
m <- 30
```

## Optimization 
Define initial values for non-relevance parameters only because relevance parameters will be assigned non-zero initial values in smaller batches.
```{r}
sigmasqInit <- 0.25
tausqInit <- 0.01^2
lambdaVec <- c(0, 1, 10, 100)
lb_nonrel_parms <- c(0.01^2, 0.01^2) 
```
`lb_nonrel_parms` defines the lower limit of $\sigma^2$ and $\tau^2$ during the optimiztion using `quad_cdsc_L1`. Next, we need to select `lambda` based on cross-validation. To use `cross_valid`, we need to define an estimation function, `est_func` and a prediction function, `pred_func`.
```{r}
opt_func_wrapper <- function(X, ...) {
  quad_cdsc_L1(start_parms = X, ...)
}
est_func <- function(lambda, locs, X, y, NNarray)
{
  objfun <- function(theta, locs){
    GpGp::vecchia_profbeta_loglik(theta, "matern25_scaledim_relevance",
                                  y, X, locs, NNarray)
  }
  objfun_gdfm <- function(theta, locs){
    GpGp::vecchia_profbeta_loglik_grad_info(theta, "matern25_scaledim_relevance",
                                            y, X, locs, NNarray)
  }
  batch_start(nonrel_parms = c(sigmasqInit, tausqInit), d = d, batch_sz = 20, 
              extra_batch = 0, opt_fun = opt_func_wrapper, likfun = objfun, 
              likfunGDFIM = objfun_gdfm, locs = locs, p = 1, lambda = lambda, 
              epsl = 1e-3, silent = T, max_iter = cvInnerIter, lb_nonrel_parms = lb_nonrel_parms)
}
pred_func <- function(rslt, locs_pred, X_pred, locs_obs, X_obs, y_obs)
{
  idxPosiSub <- rslt$covparms[2 : (length(rslt$covparms) - 1)] > 0
  idxPosi <- c(T, idxPosiSub, T)
  rslt$covparms <- rslt$covparms[idxPosi]
  if(sum(idxPosiSub) == 0)
    return(c(X_pred %*% rslt$betahat))
  GpGp::predictions(fit = rslt, locs_pred = locs_pred[, idxPosiSub], X_pred = X_pred, 
                    y_obs = y_obs, locs_obs = locs_obs[, idxPosiSub], X_obs = X_obs, 
                    covfun_name = "matern25_scaledim_relevance")
}
```
For cross-validation, I did it two times. The second time is only different from the first time in terms of location ordering.
```{r}
startTime <- Sys.time()
cvInnerIter <- 20
cvOuterIter <- 2
theta <- c(sigmasqInit, rep(1, d), tausqInit) # used for initial ordering of locs
for(i in 1 : cvOuterIter)
{
  locsScal <- locs %*% diag(theta[2 : (d + 1)])
  odr <- GpGp::order_maxmin(locsScal)
  yOdr <- y[odr]
  locsOdr <- locs[odr, ]
  XOdr <- X[odr, , drop = F]
  # Compute CV loss
  loss <- rep(NA, length(lambdaVec))
  idxRnd <- sample(c(1 : n), n, F) # used for sampling in the cross-validation
  for(j in 1 : length(lambdaVec))
  {
    loss[j] <- cross_valid(est_func, pred_func, crit_MSE, locsOdr, XOdr, yOdr, 
                           m, lambdaVec[j], 5, idxRnd)
    cat(lambdaVec[j], ": loss ", loss[j], "\n")
  }
  lambda <- lambdaVec[which.min(loss)]
  cat("CV", i, ": best lambda = ", lambda, "\n")
  # Fit with all data
  NNarray <- GpGp::find_ordered_nn(locsOdr, m = m)
  theta <- est_func(lambda, locsOdr, XOdr, yOdr, NNarray)$covparms
  cat("CV", i, ": parms = ", theta, "\n")
}
endTime <- Sys.time()
cat("Cross-validation used", as.numeric(endTime - startTime), "seconds\n")
```
The final fit inherits the previously computed `theta`. 
```{r}
startTime <- Sys.time()
crtIter <- 1
maxIter <- 100
while(maxIter >= crtIter)
{
  locsScal <- locs %*% diag(theta[2 : (d + 1)])
  odr <- GpGp::order_maxmin(locsScal)
  yOdr <- y[odr]
  locsOdr <- locs[odr, ]
  XOdr <- X[odr, , drop = F]
  NNarray <- GpGp::find_ordered_nn(locsOdr, m = m)
  # Define functions for parameter estimation in the outer loop
  objfun <- function(theta, locsOdr){
    cat("v\n")
    GpGp::vecchia_profbeta_loglik(theta, "matern25_scaledim_relevance",
                                            yOdr, XOdr, locsOdr, 
                                            NNarray)
  }
  objfun_gdfm <- function(theta, locsOdr){
    cat("g\n")
    GpGp::vecchia_profbeta_loglik_grad_info(theta, "matern25_scaledim_relevance",
                                            yOdr, XOdr, locsOdr, 
                                            NNarray)
  }
  
  theta <- quad_cdsc_L1(objfun, objfun_gdfm, locsOdr, 1, theta, lambda, 1e-3, silent = T, 
               max_iter = min(crtIter, maxIter - crtIter + 1), max_iter2 = 40, 
               lb_nonrel_parms = lb_nonrel_parms)$covparms
  cat("quad_cdsc_L1 fit iter", crtIter, ": estimated parms = ", theta, "\n")
  crtIter <- crtIter + min(crtIter, maxIter - crtIter + 1)
}
endTime <- Sys.time()
cat("Final estimates are", theta, "\n")
cat("Time used for final estimation is", as.numeric(endTime - startTime), "seconds\n")
```














