---
title: "summary4"
author: "Jian Cao"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

In this markdown, I would like to show an effective and efficient way for variable selection when $d$ is big, e.g., $1000$. This method sequentially does:

* estimate with `quad_cdsc_L1` and `batch_start` without penalty
* downsize `locs` and `theta`
* cross-validation with `locs` in a much lower dimension and find the best `lambda`
* final estimation with the best `lambda`

Compared with applying cross-validation directly, this method should be tremendously faster:

* cross-validation in `d = 1000` dimensions is estimated to take 15 hours for each `lambda`
* fitting without penalty is guaranteed to produce the most non-zero relevance parameters

## Preparation
Need to install a modified version of the GpGp R package.
```{r install the moderated GpGp Pkg}
devtools::install_github("https://github.com/SamCao1991/GpGp.git")
library(GpGp)
```
Need to load the `quad_cdsc_L1`, the `batch_start`, and the `cross_valid` functions.
```{r load relevant function}
source("quad_cdsc_L1.R")
source("cross_valid.R")
source("batch_start.R")
```
Notice that for `batch_start`, I used the `seq` function to choose relevance parameters so that the first 5 relevance parameters are chosen in the first 5 iterations, respectively, which more or less avoids `cheating' of incoporating the true relevance parameters in the same batch.

## Simulate GP in $\mathbb{R}^{1000}$
```{r simulate GP in d dimensions}
set.seed(123)
n <- 1e4
d <- 1e3
r <- c(10, 5, 2, 1, 0.5, rep(0, d - 5))
sigmasq <- 1.0 # variance
tausq <- 0.05^2 # nugget
locs <- lhs::randomLHS(n, d)
locs <- locs * outer(rep(sqrt(n), n), 1 / sqrt(colSums(locs^2)))
covM <- GpGp::matern25_scaledim_relevance(c(sigmasq, r, tausq), locs)
cholM <- t(chol(covM))
y <- as.vector(cholM %*% rnorm(n))
X <- matrix(1, n, 1)
rm(covM, cholM)
```
Notice that we still have only 5 non-zero relevance parameters, the first five. Define the conditioning size `m`.
```{r conditioning size}
m <- 100
```

## Optimization 
Define initial values for non-relevance parameters only because relevance parameters will be assigned non-zero initial values in smaller batches.
```{r init val for non-rel parms-penalty parms-opt bounds}
sigmasqInit <- 0.25
tausqInit <- 0.01^2
lambdaVec <- c(0, 1, 10, 100, 1000, 10000)
lb_nonrel_parms <- c(0.01^2, 0.01^2) 
```
`lb_nonrel_parms` defines the lower limit of $\sigma^2$ and $\tau^2$ during the optimiztion using `quad_cdsc_L1`. Next, we define `est_func` and `pred_func` to be used in `cross_valid`. `opt_func_wrapper` is a wrapper that changes the `start_parms` argument of `quad_cdsc_L1` to the first position.
```{r funcs for batch_start and cross-valid}
opt_func_wrapper <- function(X, ...) {
  quad_cdsc_L1(start_parms = X, ...)
}
est_func <- function(lambda, locs, X, y, NNarray, 
                     thetaInit = c(sigmasqInit, rep(0, d), tausqInit))
{
  objfun <- function(theta, locs){
    GpGp::vecchia_profbeta_loglik(theta, "matern25_scaledim_relevance",
                                  y, X, locs, NNarray)
  }
  objfun_gdfm <- function(theta, locs){
    GpGp::vecchia_profbeta_loglik_grad_info(theta, "matern25_scaledim_relevance",
                                            y, X, locs, NNarray)
  }
  batch_start(nonrel_parms = thetaInit[-c(2 : (d + 1))], d = d, batch_sz = 20, 
              extra_batch = 0, opt_fun = opt_func_wrapper, 
              rel_parms = thetaInit[c(2 : (d + 1))], likfun = objfun, 
              likfunGDFIM = objfun_gdfm, locs = locs, p = 1, lambda = lambda, 
              epsl = 1e-3, silent = T, max_iter = 20, lb_nonrel_parms = lb_nonrel_parms)
}
pred_func <- function(rslt, locs_pred, X_pred, locs_obs, X_obs, y_obs)
{
  idxPosiSub <- rslt$covparms[2 : (length(rslt$covparms) - 1)] > 0
  idxPosi <- c(T, idxPosiSub, T)
  rslt$covparms <- rslt$covparms[idxPosi]
  if(sum(idxPosiSub) == 0)
    return(c(X_pred %*% rslt$betahat))
  GpGp::predictions(fit = rslt, locs_pred = locs_pred[, idxPosiSub, drop = F], X_pred = X_pred, 
                    y_obs = y_obs, locs_obs = locs_obs[, idxPosiSub, drop = F], X_obs = X_obs, 
                    covfun_name = "matern25_scaledim_relevance")
}
```
The first step of variable selection is to do `batch_start` with `lambda = 0` for two times. The second time does the same as the first except with a different ordering of the locations. The goal here is to shrink the size of `d` while avoiding the high complexity of cross-validation. For this step, we can utilize the `est_func` as well.
```{r two times batch_start}
startTime <- Sys.time()

theta <- c(sigmasqInit, rep(1, d), tausqInit) # used for initial ordering of locs
for(i in 1 : 2)
{
  if(all(theta[2 : (d + 1)] == 0))
    stop("All range parameters are zero. Cannot perform reordering\n")
  locsScal <- locs %*% diag(theta[2 : (d + 1)])
  odr <- GpGp::order_maxmin(locsScal)
  yOdr <- y[odr]
  locsScalOdr <- locsScal[odr, , drop = F]
  locsOdr <- locs[odr, , drop = F]
  XOdr <- X[odr, , drop = F]
  NNarray <- GpGp::find_ordered_nn(locsScalOdr, m = m)
  
  optObj <- est_func(0, locsOdr, XOdr, yOdr, NNarray)
  theta <- optObj$covparms
}

endTime <- Sys.time()
cat("Two times of fitting without penalty used", 
    as.numeric(difftime(endTime, startTime, units = "secs")), "seconds\n")
```
See which indices of `theta` are greater than zero and what are their values.
```{r check first round fitting}
which(theta > 0)
theta[theta > 0]
```
The results seems very good. Without penalty, the result is already very good. Now I downsize `locs` to make the cross validation much faster. 
```{r downsizing locs}
idxLocsZero <- theta[2 : (d + 1)] == 0
theta <- theta[c(T, theta[2 : (d + 1)] > 0, T)]
locs <- locs[, !idxLocsZero]
d <- d - sum(idxLocsZero)
```
For cross-validation, the initial values for `theta` inherits those from the above fitting without penalty.
```{r cross-validation}
startTime <- Sys.time()

cvIter <- 20
locsScal <- locs %*% diag(theta[2 : (d + 1)])
odr <- GpGp::order_maxmin(locsScal)
yOdr <- y[odr]
locsScalOdr <- locsScal[odr, , drop = F]
locsOdr <- locs[odr, , drop = F]
XOdr <- X[odr, , drop = F]
NNarray <- GpGp::find_ordered_nn(locsScalOdr, m = m + ceiling(n / 5))
# Compute CV loss
loss <- rep(NA, length(lambdaVec))
idxRnd <- sample(c(1 : n), n, F) # used for sampling in the cross-validation
for(j in 1 : length(lambdaVec))
{
  loss[j] <- cross_valid(est_func, pred_func, crit_MSE, locsOdr, NNarray, 
                         XOdr, yOdr, m, lambdaVec[j], 5, idxRnd, 
                         thetaInit = theta)
  cat(lambdaVec[j], ": loss ", loss[j], "\n")
}
lambda <- lambdaVec[which.min(loss)]
cat("CV: best lambda = ", lambda, "\n")

endTime <- Sys.time()
cat("Cross-validation used", as.numeric(difftime(endTime, startTime, units = "secs")), "seconds\n")
```
The final fit inherits the previously computed `theta` without penalty. 
```{r}
startTime <- Sys.time()
crtIter <- 1
maxIter <- 100
while(maxIter >= crtIter)
{
  locsScal <- locs %*% diag(theta[2 : (d + 1)])
  odr <- GpGp::order_maxmin(locsScal)
  yOdr <- y[odr]
  locsScalOdr <- locsScal[odr, , drop = F]
  locsOdr <- locs[odr, , drop = F]
  XOdr <- X[odr, , drop = F]
  NNarray <- GpGp::find_ordered_nn(locsScalOdr, m = m)
  # Define functions for parameter estimation in the outer loop
  objfun <- function(theta, locsOdr){
    GpGp::vecchia_profbeta_loglik(theta, "matern25_scaledim_relevance",
                                            yOdr, XOdr, locsOdr, 
                                            NNarray)
  }
  objfun_gdfm <- function(theta, locsOdr){
    GpGp::vecchia_profbeta_loglik_grad_info(theta, "matern25_scaledim_relevance",
                                            yOdr, XOdr, locsOdr, 
                                            NNarray)
  }
  
  theta <- quad_cdsc_L1(objfun, objfun_gdfm, locsOdr, 1, theta, lambda, 1e-3, silent = T, 
               max_iter = min(crtIter, maxIter - crtIter + 1), max_iter2 = 40, 
               lb_nonrel_parms = lb_nonrel_parms)$covparms
  cat("quad_cdsc_L1 fit iter", crtIter, ": estimated parms = ", theta, "\n")
  crtIter <- crtIter + min(crtIter, maxIter - crtIter + 1)
}
endTime <- Sys.time()
cat("Final estimates are", theta, "\n")
cat("Time used for final estimation is", as.numeric(difftime(endTime, startTime, units = "secs")), "seconds\n")
```














