---
title: "summary"
author: "Jian Cao"
date: "December 16, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Experiment Summary

There are 20 spatial dimensions in total. The relevance (inverse range) parameters for the 20 dimensions are $10, 5, 2, 1, 0.5, 0, \ldots, 0$. $\sigma^2$ is $1.0$, the smoothness parameter $\nu$ is $2.5$ and $\tau^2$ is $0.05^2$. Based on the notations in `GpGp`, $\sigma^2 \tau^2$ is the nugget effect. $10^4$ observations are generated in the 20-dimensional space based on Latin Hypercube Samples.

The initial values are:
\begin{align*}
  \sigma^2 &\rightarrow 0.25, \\
  \tau^2 &\rightarrow 0, \\
  r_1, \ldots, r_{20} &\rightarrow 1.
\end{align*}
$\nu = 2.5$ is considered known. The L1 penalty parameter $\lambda$ is $1.0$. The conditioning dimension $m$ in the Vecchia approximation is $30$.

For the second-order approximation with coordinate descent method:

* Maximum iteration is 100
* Reordering and nearest neighbors are computed every $1, 2, 4, 8, 16, \ldots$ iterations

For Fisher scoring:

* Same as above
* The initial value for $\tau^2$ has to be positive for the inverse link function

For L-BFGS-B from `optim`:

* The same as the second-order approximation with coordinate descent method

For forward selection:

* The penalty parameter is not needed
* Likelihood is used for selecting variables (among the 20 spatial dimensions)
* BIC is used as the stopping criteria, i.e., stop if BIC not decreasing
* In each model estimation, the same method as L-BFGS-B from `optim` is used
* The lower bound for $\tau^2$ should be above zero, e.g., $0.01^2$ to avoid singularity

## Modifying `GpGp` for relevance parameters
Notice that `GpGp` package needs to be modified to compute the gradient and FIM relative to the relevance parameters. In fact, using the relevance parameters may be more stable as the gradient and the FIM are still valid when (not all) relevance parameters are zero.

## Results

```{r, echo = TRUE}
load("exp1_fwd_result.RData")
load("exp1_quadL1_result.RData")
load("FSRel_result.RData")
load("LBFGSB_result.RData")
table <- data.frame(matrix(0, 23, 4))
row.names(table) <- c("#Eval", "sigmasq", paste0("r", c(1 : 20)), "tausq")
colnames(table) <- c("Quad Cord Desc", "Fisher Scoring", "L-BFGS-B", "Forward Selection")
table$`Quad Cord Desc` <- c(quadL1Rslt$neval, quadL1Rslt$parms)
table$`Fisher Scoring` <- c(FSRelRslt$neval, FSRelRslt$parms)
table$`L-BFGS-B` <- c(LBFGSBRslt$neval, LBFGSBRslt$parms)
table$`Forward Selection` <- c(sum(fwdRslt$neval), fwdRslt$parms[1], rep(0, 20), 
                               fwdRslt$parms[length(fwdRslt$parms)])
table$`Forward Selection`[2 + fwdRslt$bstVar] <- fwdRslt$parms[2 : (1 + length(fwdRslt$bstVar))]
table
```
Findings:

* Second-order approximation with coordinate descent delivers exact zero and hence variable selection but the convergence isn't the fastest
* Fisher scoring uses the minimum number of evaluations of the objective function but the parameters are not exactly zero and it warns of large conditional numbers 
* L-BFGS-B worse than the previous two. 
    * Perhaps convergence is not reached under 100 iterations or the BFGS method also has issue in forcing variables to zero
    * The maximum number of iteration is not effectively implemented as the number of evaluations is 135
    * Parameter estimation is not ideal
* Forward selection correctly selects variables but the cost is huge, using 15,618 evaluations
