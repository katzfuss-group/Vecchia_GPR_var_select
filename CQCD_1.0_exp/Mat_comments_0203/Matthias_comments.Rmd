---
title: "Response to Matthias Comments"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

>  If I understand correctly, your 4th bullet is similar to what I had in mind. When d is very large, it is not feasible to start with small or zero lambda and initialize all relevances as nonzero. Instead, I think we need to start with a large lambda and most relevances equal to zero, and hopefully be able to optimize while exploiting sparsity. Then we could start decreasing lambda gradually to allow less sparse solutions, but (again, hopefully) stop this procedure before we get to a small lambda and very dense solution (e.g., because the cross validation indicates that the solutions are getting worse).

When using the relevance parameters, I agree that we need to start with most relevances equal to zero. In this case, we can exploit the sparsity quite well, i.e., computing the gradient and FIM w.r.t. non-zero relevance parameters. 

As for decreasing lambda to allow less sparsity, I am not sure if this can be done when using the relevance parameters since zero-valued parameters will stay at zero. However, in terms of variable selection, I am considering the following procedure:

* Do the estimation with all data and zero penalty, which already gives a sparse model. Notice that the estimation here involves `batch_start` that iterates between:
    + add one to a small number (20) of relevance parameters
    + run `quad_cdsc_L1` for a maximum of 20 iterations
* Discard the dimensions with zero relevance parameters
* Run cross-validation in the remaining dimensions, since $\lambda = 0$ should give the least sparse model
* Find the best `lambda` value based on some criteria, currently its MSE, which is not ideal
* Fit the model with all data and the best `lambda`

The above procedure is implemented in `summary`, which seems very efficient and effective in our simulation study with `n = 1e4` and `d = 1e3`.

> Related to that, you said on Overleaf that "relevance parameters that already reach zero will stay at zero." As I wrote in a previous email:
"What if we did the optimization wrt the square root of the relevances, say $s_l = (r_l)^{1/2}$? Would the derivative wrt to $s_l$ at $s_l=0$ be nonzero? Would that work?
If not, do you have any other suggestions for how to jump-start? This will be crucial when we do the optimization sequentially for different penalty parameters, starting with the empty model."

I think that $s_l = (r_l)^{1/2} \Rightarrow r_l = s_l^2$.
$$
  \frac{\partial l}{\partial s_l} = \frac{\partial l}{\partial r_l} \frac{\partial r_l}{\partial s_l} = 0 \times 2 s_l = 0
$$
If we consider $s_l = (r_l)^{2}$, assuming $d_{ij}$ is the distance between the $i$-th and $j$-th locations under the `scaled' location coordinates.
\begin{align*}
  \frac{\partial \boldsymbol{\Sigma_{ij}}}{\partial s_l} &= \frac{\partial \boldsymbol{\Sigma_{ij}}}{\partial d_{ij}} \frac{\partial d_{ij}}{\partial r_l} \frac{\partial r_l}{\partial s_l} \\
  &= \frac{\partial \boldsymbol{\Sigma_{ij}}}{\partial d_{ij}} \frac{c_1 \cdot r_l}{d_{ij}} \frac{1}{2} \frac{1}{r_l} \\
  &= c_2 \cdot \frac{\partial \boldsymbol{\Sigma_{ij}}}{\partial d_{ij}} \frac{1}{d_{ij}}
\end{align*} 
Hence, $\frac{\partial \boldsymbol{\Sigma_{ij}}}{\partial s_l}$ should be non-zero. However, it requires further experiment to see if they can return to non-zero values from zero.

As for jump-start, the idea is summarized in `batch_start` introduced above.

> Also related to 1), I'm still not sure how you know how to pre-determine reasonable lambda values. For example, how large would lambda have to be to get a completely sparse solution? How did you come up with the value of 10 in your example? As I wrote in a previous email:
"What exact range of lambda values are you proposing? I guess the idea would be to have a lambda grid whose upper endpoint is large enough (i.e., strong enough penalization) to obtain a completely empty model, and whose lower endpoint is small enough so that all covariates are included. Is that guaranteed with your suggestion?"

I think this discussion may be discussed later or is less needed since zero penalty, based on the simulation study, already produces a sparse model. Finding a $\lambda$ that makes all relevance parameters zero is also not difficult, e.g., $\lambda = n$ or we can exponential increase it until the model is empty.

After dimension reduction, cross-validation is tremendously faster.

> By the way, in your code it looks like you are doing the maximin ordering in the scaled input space, but the nearest-neighbor search in the original input space. I recommend doing both in the scaled space. That is what we did in SVecchia (https://github.com/katzfuss-group/scaledVecchia/blob/master/vecchia_scaled.R).

Yep, thanks! I need to change the code to base the nearest-neighbor searching on the ordered scaled locations.

> In addition, you may want to try a larger m, to see if that leads to improved solutions. $m = 30$ is reasonably large in 2 input dimensions, but it is probably too small to get good approximation accuracy in 20 or even 100 dimensions.

Sure, I used $m = 100$ in `summary`.

> This is not a high priority right now, but eventually it would be interesting to explore what happens when the covariates are correlated (as will likely often be the case in "real" regression settings) and to compare the methods in terms of prediction scores (e.g., RMSE and log score).

OK, so let's consider this later. To generate more correlated columns in `locs`, I guess that it means not using the hyper Latin cude generating method.

> Regarding Joe's suggestion of a cutoff for Fisher scoring, we provided that option (by setting the function argument "select" to a finite value) in SVecchia.

Yes, good to know ) Although the implementation is outside the Fisher_scoring algorithm, the computation savings and the effectiveness of variable selection should be similar. However, two questions may need consideration:

* How to choose the threshold $r_0$
* This means that once a relevance parameter reaches $r_0$, it will stay at zero, which is stricter than that once a relevance parameter reaches $0$, it will stay at zero

## Conclusion -- 0215

* The first-stage fitting without penalty is highly beneficial for cross-validation. In fact, based on the simulation result, cross-validation may not seem very important. Of course, we should check the case when the covariates are correlated 
* Using the predition from `GpGp` and MSE as the criteria for cross-validation is not optimal. As shown in `summary`, `lambda = 100` is clearly not as good as `lambda = 0` in terms of the distance to the true relevance parameter values
* The reason I first did fitting without penalty is 
    + Observation that sparsity already appears when `lambda = 0`
    + Cross-validation when `n = 1e4` and `d = 1e3` is very expensive




