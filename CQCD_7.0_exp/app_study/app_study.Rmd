---
title: "Application Study"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Method 
Bridge penalty solution path, forward-backward selection, and CQCD. Predictor selection and CQCD are run with mini-batching.

## Preparation
Clean the environment
```{r clean environment}
rm(list = ls())
```
Need to install a modified version of the GpGp R package.
```{r install the moderated GpGp Pkg}
devtools::install_github("https://github.com/SamCao1991/GpGp.git")
library(GpGp)
library(FNN)
library(parallel)
library(scoringRules)
if(file.exists("CQCD.R")){
  source("CQCD.R")
}else{
  source("../CQCD.R")
}
set.seed(123)
```
Pre-processing.
```{r divide and normalize, eval = T}
dataFn <- "slice_localization_data.csv"
if(file.exists(dataFn)){
  dataSet <- read.csv(dataFn, header = F)
}else{
  dataSet <- read.csv(paste0("../../data/", dataFn), header = F)
}

colIDObj <- 386
d <- ncol(dataSet) - 1
tmp <- dataSet[, d + 1]
dataSet[, d + 1] <- dataSet[, colIDObj]
dataSet[, colIDObj] <- tmp

dummyIdx <- c(1)
if(length(dummyIdx) > 0)
  dataSet <- dataSet[, - dummyIdx]

constIdx <- which(apply(dataSet, 2, sd) == 0) 
if(length(constIdx) > 0)
  dataSet <- dataSet[, - constIdx]
d <- ncol(dataSet) - 1
dataSet[, 1 : d] <- 
  apply(dataSet[, 1 : d], MARGIN = 2, 
        function(x){x <- (x - mean(x)) / sd(x)})

dFake <- 1e3 - d
if(dFake > 0){
  predFake <- matrix(rnorm(dFake * nrow(dataSet)), nrow(dataSet), dFake, 
                     dimnames = list(row = NULL, 
                                     col = paste0("FV", (d + 1) : (d + dFake))))
  dataSet <- cbind(dataSet[, 1 : d], predFake, dataSet[, d + 1])
  d <- d + dFake
  rm(predFake)
}
```
Split Dataset
```{r split dataset, eval = T}
nTrain <- 40177
nTest <- nrow(dataSet) - nTrain
dataSetTrain <- dataSet[1 : nTrain, ]
dataSetTest <- dataSet[(nTrain + 1) : nrow(dataSet), ]

nOOS <- min(5000, floor(nTrain / 2))
dataSetTrainTestIdx <- sample(1 : nTrain, nOOS, F)
dataSetTrainTest <- dataSetTrain[dataSetTrainTestIdx, , drop = F]
dataSetTrain <- dataSetTrain[- dataSetTrainTestIdx, , drop = F]
nTrain <- nTrain - nOOS

rm(colIDObj, tmp)
```
Reformat data
```{r reformat data, eval = T}
locsTrain <- as.matrix(dataSetTrain[, 1 : d])
locsOOS <- as.matrix(dataSetTrainTest[, 1 : d])
locsTest <- as.matrix(dataSetTest[, 1 : d])
# standardize y
meanYTrain <- mean(dataSetTrain[, d + 1])
sdYTrain <- sd(dataSetTrain[, d + 1])
yTrain <- (dataSetTrain[, d + 1] - meanYTrain) / sdYTrain
yOOS <- (dataSetTrainTest[, d + 1] - meanYTrain) / sdYTrain
yTest <- (dataSetTest[, d + 1] - meanYTrain) / sdYTrain
```
Some EDA for `y`
```{r EDA y, eval = F}
library(ggplot2)
hist(c(yTrain, yOOS, yTest))
```
Function for computing the approximate OOS score, assuming the squared relevance parameterized Matern25 kernel.
```{r OOS score, eval = T}
OOS_score <- function(theta)
{
  idxLocsRel <- which(theta[2 : (d + 1)] > 0)
  dRel <- length(idxLocsRel)
  locsRel <- rbind(locsTrain, locsOOS)[, idxLocsRel, drop = F]
  thetaRel <- theta[c(1, idxLocsRel + 1, d + 2)]
  locsRelScal <- locsRel %*% 
    diag(sqrt(thetaRel[2 : (dRel + 1)]), dRel, dRel)
  yTtl <- c(yTrain, yOOS)
  NNarray <- get.knnx(locsRelScal[1 : nTrain, , drop = F],
                      locsRelScal[(nTrain + 1) : (nTrain + nOOS), , drop = F], 
                      m)$nn.index
  NNarray <- cbind(NNarray, (nTrain + 1) : (nTrain + nOOS))
  mus <- rep(NA, nOOS)
  sds <- rep(NA, nOOS)
  for(i in 1 : nOOS){
    NN <- NNarray[i, ]
    K <- matern25_scaledim_sqrelevance(thetaRel, locsRel[NN, , drop = F])
    L <- t(chol(K))
    mus[i] <- L[m + 1, 1 : m] %*% 
      forwardsolve(L[1 : m, 1 : m], yTrain[NN[1 : m]])
    sds[i] <- L[m+1, m+1]
  }
  # mean(crps_norm(y = yOOS, mean = mus, sd = sds))
}
```

## Using just mean
```{r compare with using just mean, eval = F}
var(dataSetTest[, d + 1]) * nrow(dataSetTest)
```

## PCR.
```{r PCA linear reg, eval = F}
library(pls)
colnames(dataSetTrain)[d + 1] <- "obj"
pcrMdl <- pcr(obj ~ ., data = dataSetTrain, ncomp = 20)
PCPred <- predict(pcrMdl, dataSetTest[, 1 : d], ncomp = 20)
sum((PCPred - dataSetTest[, d + 1])^2)
```

## Lasso regression.
```{r Lasso regression, eval = F}
library(glmnet)
lassoPath <- glmnet(x = dataSetTrain[, 1 : d], y = dataSetTrain[, d + 1],
                         alpha = 1, intercept = T)
plot(lassoPath)
lassoOOS <- matrix(lassoPath$a0, nrow = nOOS, 
                   ncol = ncol(lassoPath$beta), byrow = T) +
  as.matrix(dataSetTrainTest[, 1 : d]) %*% lassoPath$beta
OOSRMSE <- apply(lassoOOS, MARGIN = 2, function(x){
  sum((x - dataSetTrainTest[, d + 1])^2)
  })
OOSRMSEChg <- c(1, OOSRMSE[2 : length(OOSRMSE)] / 
                  OOSRMSE[1 : (length(OOSRMSE) - 1)] - 1)
idxOptLasso <- 50
lassoPred <- apply(dataSetTest[, 1 : d], 1, 
                   function(x){sum(x * lassoPath$beta[, idxOptLasso]) +
                       lassoPath$a0[idxOptLasso]})
sum((lassoPred - dataSetTest[, d + 1])^2)
```

## VREG
`m` and `k`
```{r m and k, eval = T}
m <- 100
k <- 3
```
Batch size
```{r batch size, eval = T}
batchSzCQCD <- min(2e3, nTrain)
batchSzGrad <- min(2e3, nTrain)
```
Bridge penalty with `gamma = 1/4`
```{r penalty, eval = T}
gamma <- 1/4
penfun <- function(theta){
  lambda * sum(theta[-c(1, length(theta))]^(gamma))
}
dpenfun <- function(theta){
  r <- theta[-c(1, length(theta))]
  rpen <- lambda * r^(gamma - 1) * gamma
  rpen[r < 1e-10] <- lambda * (1e-10)^(gamma - 1) * gamma
  c(0, rpen, 0)
}
ddpenfun <- function(theta){
  diag(rep(0, length(theta)))
}
```
Function for model fitting given lambda
```{r fit model for lambda, eval = T}
forward_backward <- function(theta0, idx0, gradObj0)
{
  theta <- theta0
  idx <- idx0
  gradObj <- gradObj0
  while(T)
  {
    odrDec <- order(gradObj$grad[2 : (d + 1)], decreasing = T)
    if(length(idx) == 0)
      idxNew <- 1 + odrDec[1 : k]
    else
      idxNew <- c(idx, 1 + setdiff(odrDec, idx - 1)[1 : k])
    idxSel <- setdiff(idxNew, idx)
    cat("Selected var:", idxSel, "\n")
    # opt with SR parameters
    thetaRel <- theta[c(1, idxNew, 2 + d)] 
    thetaRel[(length(idx) + 2) : (length(idx) + k + 1)] <- 1
    # maximin order and NNarray
    locsRel <- locsTrain[, idxNew - 1, drop = F]
    locsRelScal <- locsRel %*% 
      diag(sqrt(thetaRel[2 : (1 + length(idxNew))]), 
           length(idxNew), length(idxNew))
    odr <- 1 : nTrain # GpGp::order_maxmin(locsRelScal)
    yOdr <- yTrain[odr]
    locsRelScalOdr <- locsRelScal[odr, , drop = F]
    locsOdr <- locsTrain[odr, , drop = F]
    NNarray <- GpGp::find_ordered_nn(locsRelScalOdr, m = m)
    # extract relevant col in locsOdr and lb
    locsOdrRel <- locsOdr[, idxNew - 1, drop = F] 
    lbRel <- lb[c(1, idxNew, 2 + d)] 
    objfun <- function(theta, batchIdx){
      # NNarraySubList <- mclapply(batchIdx, function(i){
      #   get.knnx(locsRelScalOdr[1 : i, , drop = F], 
      #            locsRelScalOdr[i, , drop = F], 
      #            min(m + 1, i))$nn.index
      #   }, mc.cores = 32) 
      # NNarraySubListApd <- lapply(NNarraySubList, function(x){
      #   if(length(x) < m + 1){
      #     return(c(x, rep(NA, m + 1 - length(x))))
      #   }else
      #     return(x)
      #   })
      # NNarraySub <- matrix(unlist(NNarraySubListApd), 
      #                      length(batchIdx), m + 1, byrow = T)
      likObj <- GpGp::vecchia_meanzero_loglik(theta, 
                                              "matern25_scaledim_sqrelevance",
                                              yOdr, locsOdrRel, 
                                              NNarray[batchIdx, , drop = F])
      likObj$loglik <- likObj$loglik - penfun(theta)
      likObj
    }
    objfun_gdfm <- function(theta, batchIdx){
      # NNarraySubList <- mclapply(batchIdx, function(i){
      #   get.knnx(locsRelScalOdr[1 : i, , drop = F], 
      #            locsRelScalOdr[i, , drop = F], 
      #            min(m + 1, i))$nn.index
      #   }, mc.cores = 32) 
      # NNarraySubListApd <- lapply(NNarraySubList, function(x){
      #   if(length(x) < m + 1){
      #     return(c(x, rep(NA, m + 1 - length(x))))
      #   }else
      #     return(x)
      #   })
      # NNarraySub <- matrix(unlist(NNarraySubListApd), 
      #                      length(batchIdx), m + 1, byrow = T)
      likObj <- GpGp::vecchia_meanzero_loglik_grad_info(theta, 
                                              "matern25_scaledim_sqrelevance",
                                              yOdr, locsOdrRel, 
                                              NNarray[batchIdx, , drop = F])
      likObj$loglik <- likObj$loglik - penfun(theta)
      likObj$grad <- likObj$grad - dpenfun(theta)
      likObj$info <- likObj$info + ddpenfun(theta)
      likObj
    }
    timeObj <- system.time(
      optObj <- CQCD_stochastic(objfun, objfun_gdfm, nTrain, batchSzCQCD,
                                thetaRel, 
                                maxIterOut = 100, maxIterIn = 40, lb = lbRel,
                                arg_check = arg_check_SR))
    # Take out zero relevance
    thetaRel <- optObj$covparms
    idxLocZero <- which(thetaRel[2 : (1 + length(idxNew))] == 0)
    if(length(idxLocZero) > 0)
    {
      idxDesel <- idxNew[idxLocZero]
      cat("Predictor", idxDesel, "are zerod out \n")
      idxNew <- idxNew[-idxLocZero]
      thetaRel <- thetaRel[-(idxLocZero + 1)]
    }
    theta <- rep(0, length(theta))
    theta[c(1, idxNew, length(theta))] <- thetaRel
    idx <- idxNew
    # compute grad with thetaNew
    batchIdx <- sample(x = 1 : nTrain, size = batchSzGrad, replace = F)
    timeObj <- system.time(
      gradObj <- 
        GpGp::vecchia_meanzero_loglik_grad_info(theta, 
                                                "matern25_scaledim_sqrelevance",
                                                yOdr, locsOdr, 
                                                NNarray[batchIdx, , drop = F]))
    # break condition
    if((length(idxLocZero) > 0 && all(idxSel %in% idxDesel)) || 
       length(idx) > 40)
      break
  }
  cat("lambda =", lambda, "idx =", idx, "\n\n")
  return(list(lambda = lambda, theta = theta, idx = idx, gradObj = gradObj))
}
```
Optimization initialization.
```{r opt init, eval = T}
sigmasqInit <- 0.25
tausqInit <- 0.01
theta <- c(sigmasqInit, rep(1e-8, d), tausqInit)
lb <- c(0.01^2, rep(0, d), 0.01^2) 
arg_check_SR <- function(x) {sum(sqrt(x[-c(1, length(x))])) > 1e-4}
idx <- c()
yOdr <- yTrain
locsOdr <- locsTrain
NNarray <- find_ordered_nn(locsTrain[, sample(1 : d, k), drop = F], m = m)
batchIdx <- sample(x = 1 : nTrain, size = batchSzGrad, replace = F)
gradObj <- vecchia_meanzero_loglik_grad_info(theta, 
                                            "matern25_scaledim_sqrelevance",
                                            yOdr, locsOdr, 
                                            NNarray[batchIdx, , drop = F])
lb[which.max(gradObj$grad[2 : (d + 1)]) + 1] <- 2e-4
```
Loop over lambda.
```{r loop over lambda, eval = T}
lambdaUp <- 1
lambdaVec <- c(rev(2^(seq(from = log2(1/4096), to = log2(max(0.1, lambdaUp)), 
                        by = 1))))
niter <- length(lambdaVec)
idxSet <- list()
thetaSet <- list()
scoreSet <- list()
lambdaSet <- list()
loglikSet <- list()
startTime <- Sys.time()
for(i in 1 : niter)
{
  cat("\n====================================\n")
  lambda = lambdaVec[i]
  cat("i =", i, "lambda =", lambda, "\n")
  # fit model with penalty
  optObj <- forward_backward(theta, idx, gradObj)
  theta <- optObj$theta
  idx <- optObj$idx
  gradObj <- optObj$gradObj
  # Store results
  idxSet[[i]] <- optObj$idx
  thetaSet[[i]] <- optObj$theta
  scoreSet[[i]] <- OOS_score(optObj$theta)
  lambdaSet[[i]] <- lambda
  loglikSet[[i]] <- optObj$gradObj$loglik
  if(i > 1 && scoreSet[[i]] / scoreSet[[i - 1]] > 0.99 &&
     length(setdiff(idxSet[[i]], idxSet[[i - 1]])) > 0 && 
     sum(thetaSet[[i]][2 : (d + 1)]) > 0.1)
    break
}
endTime <- Sys.time()
timeObj <- endTime - startTime
timeObj
```
Save the result.
```{r save the result, eval = T}
save(list = c("d", "idxSet", "thetaSet", "scoreSet", "lambdaSet", "loglikSet",
              "lambdaVec", "nTrain", "timeObj"), 
     file = sub("\\..*", ".RData", dataFn))
```

## Check new RMSE

```{r predict at testing locs, eval = T}
source("vecchia_scaled.R")
# load(sub("\\..*", ".RData", dataFn))
idxOpt <- max(length(thetaSet) - 1, 1)
m <- 100
fit <- list(y = yTrain, locs = locsTrain[, idxSet[[idxOpt]] - 1, drop = F], 
              covparms = thetaSet[[idxOpt]][c(1, idxSet[[idxOpt]], d + 2)], 
              X = matrix(0, nTrain, 1), 
              betahat = 0, covfun_name = "matern25_scaledim_sqrelevance", 
              trend = "zero")
predObj <- predictions_scaled(fit, locsTest[, idxSet[[idxOpt]] - 1, drop = F], 
                              m, T)
GPPred <- predObj * sdYTrain + meanYTrain
sum((GPPred - dataSetTest[, d + 1])^2)
```


















































