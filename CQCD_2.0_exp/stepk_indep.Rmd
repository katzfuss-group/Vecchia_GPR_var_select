---
title: "stepk_indep"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview
This summary uses BIC and BIC penalized objective function. The step size can be defined in `k`. Hopefully, this markdown will show the point of deselecting with `CQCD`.
```{r clean environment}
rm(list = ls())
```


## Preparation
Need to install a modified version of the GpGp R package.
```{r install the moderated GpGp Pkg}
devtools::install_github("https://github.com/SamCao1991/GpGp.git")
library(GpGp)
library(mvtnorm)
library(ggplot2)
library(RColorBrewer)
library(scales)
source("CQCD.R")
set.seed(123)
```

## Stopping condition
```{r stopping condition}
stop_cond <- function(objOld, objNew, dOld, dNew)
{
  BICOld <- log(n) * dOld + 2 * objOld
  BICNew <- log(n) * dNew + 2 * objNew
  return(BICNew > BICOld)
} 
```

## Experiment
```{r simulate GP}
n <- 1e4
d <- 100
m <- 100
k <- 5
rSq <- c(10, 5, 2, 1, 0.5, rep(0, d - 5))^2
sigmasq <- 1.0 # variance
tausq <- 0.05^2 # nugget
locs <- lhs::randomLHS(n, d)
locs <- locs * outer(rep(sqrt(n), n), 1 / sqrt(colSums(locs^2)))
covM <- GpGp::matern25_scaledim_sqrelevance(c(sigmasq, rSq, tausq), locs)
cholM <- t(chol(covM))
y <- as.vector(cholM %*% rnorm(n))
X <- matrix(1, n, 1)
rm(covM, cholM)
```

```{r forward-backward selection}
sigmasqInit <- 0.25
tausqInit <- 0.01^2
theta <- c(sigmasqInit, rep(1e-8, d), tausqInit)
lb <- c(0.01^2, rep(0, d), 0.01^2) 
arg_check <- function(x) {sum(x[-c(1, length(x))]) > 1e-10}
idx <- c()
rslt <- list()
breakIter <- NA
startTime <- Sys.time()
for(i in 1 : 10)
{
  startTimeNNarrray <- Sys.time()
  idxLocRel <- theta[2 : (d + 1)] > 0
  locsRel <- locs[, idxLocRel, drop = F]
  locsRelScal <- locsRel %*% diag(sqrt(theta[2 : (d + 1)][idxLocRel]), 
                               sum(idxLocRel), sum(idxLocRel))
  odr <- GpGp::order_maxmin(locsRelScal)
  yOdr <- y[odr]
  locsRelScalOdr <- locsRelScal[odr, , drop = F]
  locsOdr <- locs[odr, , drop = F]
  XOdr <- X[odr, , drop = F]
  NNarray <- GpGp::find_ordered_nn(locsRelScalOdr, m = m)
  endTimeNNarrray <- Sys.time()
  cat("Finding NNarray used", 
      as.numeric(difftime(endTimeNNarrray, startTimeNNarrray, units = "secs")), 
      "seconds\n")
  startTimeGrad <- Sys.time()
  gradObj <- 
    GpGp::vecchia_profbeta_loglik_grad_info(theta, 
                                            "matern25_scaledim_sqrelevance",
                                            yOdr, XOdr, locsOdr, NNarray)
  endTimeGrad <- Sys.time()
  cat("Computing gradient used", 
      as.numeric(difftime(endTimeGrad, startTimeGrad, units = "secs")), 
      "seconds\n")
  # select covariates
  odrDec <- order(gradObj$grad[2 : (d + 1)], decreasing = T)
  if(length(idx) == 0)
    idxNew <- 1 + odrDec[1 : k]
  else
    idxNew <- c(idx, 1 + setdiff(odrDec, idx - 1)[1 : k])
  rslt[[5 * (i - 1) + 1]] <- gradObj$grad[2 : (d + 1)]
  rslt[[5 * (i - 1) + 2]] <- idx - 1
  rslt[[5 * (i - 1) + 3]] <- setdiff(idxNew, idx) - 1
  cat("Selected var", setdiff(idxNew, idx), "at iter", i, "\n")
  
  # remove irrelevant predictors
  thetaRel <- theta[c(1, idxNew, 2 + d)] 
  locsOdrRel <- locsOdr[, idxNew - 1, drop = F] 
  lbRel <- lb[c(1, idxNew, 2 + d)] 
  
  # opt with SR parameters
  objfun <- function(theta){
    GpGp::vecchia_profbeta_loglik(theta, "matern25_scaledim_sqrelevance",
                                            yOdr, XOdr, locsOdrRel, 
                                            NNarray)
  }
  objfun_gdfm <- function(theta){
    GpGp::vecchia_profbeta_loglik_grad_info(theta, "matern25_scaledim_sqrelevance",
                                            yOdr, XOdr, locsOdrRel, 
                                            NNarray)
  }
  startTimeOpt1 <- Sys.time()
  optObj <- CQCD(objfun, objfun_gdfm, thetaRel, maxIterOut = 3, maxIterIn = 40, 
               lb = lbRel, arg_check = arg_check)
  endTimeOpt1 <- Sys.time()
  cat("Opt with SR used", 
      as.numeric(difftime(endTimeOpt1, startTimeOpt1, units = "secs")), 
      "seconds\n")
  thetaRel <- optObj$covparms
  # record the obj val using the previously optimized parms and the new order
  obj <- - optObj$loglikInit
  
  # opt with relevance parameters, here BIC penalty is applied
  dRel <- length(idxNew)
  thetaRel[2 : (1 + dRel)] <- sqrt(thetaRel[2 : (1 + dRel)])
  rslt[[5 * (i - 1) + 5]] <- thetaRel[2 : (1 + dRel)]
  BIC_pen <- function(theta){
    idx <- which(thetaRel[2 : (1 + dRel)] > 0) + 1
    log(n) / 2 * sum(pmin(theta[idx] / 
                          (2 * thetaRel[idx]), 1))
  }
  BIC_dpen <- function(theta){
    idx <- which(thetaRel[2 : (1 + dRel)] > 0) + 1
    dpen <- rep(0, length(theta))
    dpen[idx] <- log(n) / 2 * (theta[idx] < 2 * thetaRel[idx]) / 
      (2 * thetaRel[idx])
    dpen[is.infinite(dpen)] <- 0
    dpen
  }
  objfun <- function(theta){
    likObj <- GpGp::vecchia_profbeta_loglik(theta, "matern25_scaledim_relevance",
                                            yOdr, XOdr, locsOdrRel,
                                            NNarray)
    likObj$loglik <- likObj$loglik - BIC_pen(theta)
    likObj
  }
  objfun_gdfm <- function(theta){
    likObj <- 
      GpGp::vecchia_profbeta_loglik_grad_info(theta,
                                              "matern25_scaledim_relevance",
                                              yOdr, XOdr, locsOdrRel, NNarray)
    likObj$loglik <- likObj$loglik - BIC_pen(theta)
    likObj$grad <- likObj$grad - BIC_dpen(theta)
    likObj
  }
  startTimeOpt2 <- Sys.time()
  optObj <- CQCD(objfun, objfun_gdfm, thetaRel, maxIterOut = 5, maxIterIn = 40,
               lb = lbRel, arg_check = arg_check)
  endTimeOpt2 <- Sys.time()
  cat("Opt with rel used", 
      as.numeric(difftime(endTimeOpt2, startTimeOpt2, units = "secs")), 
      "seconds\n")
  objNew <- - optObj$loglik - BIC_pen(optObj$covparms)
  thetaRel <- optObj$covparms
  thetaRel[2 : (1 + length(idxNew))] <- thetaRel[2 : (1 + length(idxNew))]^2
  
  # Where would BIC stop
  if(is.na(breakIter) && 
     # stop_cond(obj, objNew, length(idx), length(idxNew)))
     stop_cond(obj, objNew, length(idx), length(idxNew) -
               sum(thetaRel[2 : (1 + length(idxNew))] == 0)))
    breakIter <- i
  # Take out zero relevance
  idxLocZero <- which(thetaRel[2 : (1 + length(idxNew))] == 0)
  if(length(idxLocZero) > 0)
  {
    cat("Predictor", idxNew[idxLocZero], "are zerod out \n")
    idxNew <- idxNew[-idxLocZero]
    thetaRel <- thetaRel[-(idxLocZero + 1)]
  }
  # Copy val for next iter
  idx <- idxNew
  theta <- rep(0, d + 2)
  theta[c(1, idx, d + 2)] <- thetaRel
  
  rslt[[5 * (i - 1) + 4]] <- theta
  rslt[[5 * (i - 1) + 4]][2 : (d + 1)] <- 
    sqrt(rslt[[5 * (i - 1) + 4]][2 : (d + 1)])
}
cat(idx, "\n")
cat(theta[c(1, idx, d + 2)], "\n")
endTime <- Sys.time()
cat("Time used for estimation is", as.numeric(difftime(endTime, startTime, units = "secs")), "seconds\n")
```

```{r save the result}
save.image(file = paste0("step", k, "_indep.RData"))
```

## Plot the selected var at each iter

```{r build dataframe for var selected at each iter}
mydf <- matrix(NA, d * 10, 4)
for(i in 1 : 10)
{
  mydf[((i - 1) * d + 1) : (i * d), 1] <- rslt[[5 * (i - 1) + 1]]
  mydf[((i - 1) * d + 1) : (i * d), 2] <- i
  mydf[((i - 1) * d + 1) : (i * d), 4] <- 0 # color
  mydf[(i - 1) * d + setdiff(rslt[[5 * (i - 1) + 2]], rslt[[5 * (i - 1) + 3]]), 
       1 : 2] <- NA
  mydf[(i - 1) * d + rslt[[5 * (i - 1) + 3]], 3] <- rslt[[5 * (i - 1) + 3]]
  mydf[(i - 1) * d + which(rSq > 0), 4] <- sqrt(rSq[which(rSq > 0)]) # color
}
mydf <- as.data.frame(mydf)
colnames(mydf) <- c("gradient", "iter", "label", "color")
mydf <- mydf[!is.na(mydf$gradient), ]
mydf$iter <- as.factor(mydf$iter)
mydf$color[mydf$color > 0] <- paste0("rel = ", mydf$color[mydf$color > 0])
mydf$color[mydf$color == 0] <- "rel = 0"
mydf$color <- factor(mydf$color, levels = unique(mydf$color))
```

```{r plot the var selected at each iter}
ggplot(data = mydf, aes(x = iter, y = gradient, col = factor(color),
                        label = label)) +
  # geom_text(nudge_x = 0.4, show.legend = FALSE) +
  geom_point() +
  scale_color_manual(breaks = paste0("rel = ", sqrt(unique(rSq))),
                     values = c(rainbow(length(unique(rSq)) - 1),
                                "grey")) +
  scale_y_continuous(trans = pseudo_log_trans(sigma = 4e-1),
                     breaks = c(0, 
                                signif(min(mydf$gradient) / 2, digits = 1),
                                signif(exp(seq(from = 1,
                                               to = log(max(mydf$gradient)), 
                                               length.out = 5)), 
                                       digits = 1))) +
  xlab("iteration number") +
  theme(legend.title = element_blank(), legend.position="right") +
  expand_limits(x = c(11)) +
  geom_vline(xintercept = breakIter - 0.5, lty = "dashed", col = "red") +
  geom_text(data = NULL, 
            mapping = aes(x = breakIter - 0.5, y = max(mydf$gradient), 
                          label = "BIC"), 
            col = "red", position = position_nudge(x = 0.5), cex = 5,
            show.legend = FALSE, fontface = "plain", family = "mono")
ggsave(paste0("gradient_indep_", k, ".pdf"), width = 6, height = 3.375)
```

## Plot the convergence of the non-zero rel

```{r build dataframe for convergence of non-zero rel}
nRow <- sum(sapply(c(1 : 10), 
                      function(i){length(rslt[[5 * (i - 1) + 2]]) * 2 + 
                          k * 3}))
mydf <- matrix(NA, nRow + 5, 6)
rowIdx <- 1
for(i in 1 : 10)
{
  mydf[rowIdx : (rowIdx + k - 1), 1] <- 0 # init var val
  mydf[rowIdx : (rowIdx + k - 1), 2] <- i - 1 # iter
  mydf[rowIdx : (rowIdx + k - 1), 3] <- rslt[[5 * (i - 1) + 3]] # var ID
  idxVar <- c(rslt[[5 * (i - 1) + 2]], rslt[[5 * (i - 1) + 3]])
  mydf[(rowIdx + k) : (rowIdx + k + length(idxVar) - 1), 1] <- 
    rslt[[5 * (i - 1) + 5]]
  mydf[(rowIdx + k + length(idxVar)) : 
         (rowIdx + k + 2 * length(idxVar) - 1), 1] <- 
    rslt[[5 * (i - 1) + 4]][idxVar + 1]
  mydf[(rowIdx + k) : (rowIdx + k + length(idxVar) - 1), 2] <- i - 0.5 # iter
  mydf[(rowIdx + k + length(idxVar)) : 
         (rowIdx + k + 2 * length(idxVar) - 1), 2] <- i # iter
  mydf[(rowIdx + k) : (rowIdx + k + 2 * length(idxVar) - 1), 3] <- idxVar # var ID
  
  rowIdx <- rowIdx + length(idxVar) * 2 + k
}
for(i in 1 : 5)
{
  mydf[rowIdx + i - 1, 3] <- i
  mydf[rowIdx + i - 1, 5] <- i
  mydf[rowIdx + i - 1, 6] <- sqrt(rSq[i])
}
rowIdx <- rowIdx + 5
mydf <- as.data.frame(mydf)
colnames(mydf) <- c("val", "iter", "varid", "label", "varidLabel",
                    "y_intercept")
mydf$label <- paste0("rel = ", sqrt(rSq[mydf$varid]))
mydf$varidLabel[!is.na(mydf$varidLabel)] <- 
  paste0("var ", mydf$varidLabel[!is.na(mydf$varidLabel)])
mydf$varid <- factor(mydf$varid, levels = unique(mydf$varid))
mydf$label <- factor(mydf$label, levels = unique(mydf$label))
mydf
```
Plot the convergence of parms
```{r plot the parm vals}
ggplot(data = mydf, aes(x = iter, y = val, group = varid, col = label,
                        label = varidLabel)) +
  geom_line() +
  geom_label(position = position_jitterdodge(),
             show.legend = F) +
  xlab("iteration number") +
  ylab("relevance parameters") +
  scale_color_manual(breaks = paste0("rel = ", sqrt(unique(rSq))),
                     values = c(rainbow(length(unique(rSq)) - 1),
                                "grey")) +
  # scale_color_brewer(palette = "Dark2") +
  scale_x_continuous(breaks = c(0 : 10)) +
  scale_y_continuous(trans = pseudo_log_trans(sigma = 1e-2),
                     breaks = unique(c(sqrt(rSq), 0))) +
  geom_hline(mapping = aes(col = label, yintercept = y_intercept), 
             size = 0.5, lty = "dashed") + 
  theme(legend.title = element_blank(), legend.position="right")
ggsave(paste0("fwdbwd_indep_", k, ".pdf"), width = 6, height = 3.375)
```










