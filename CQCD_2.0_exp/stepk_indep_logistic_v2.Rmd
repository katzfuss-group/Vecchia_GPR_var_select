---
title: "SR with Logistic Penalty V2.0"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Method 
In logistic penalty V1.0, for each (squared) relevance parameter, we used the penalty function.
$$
  \frac{\lambda}{1 + e^{-\lambda x}} - \frac{\lambda}{2}.
$$
Here in the second experiment with the logistic penalty, we try:
$$
  \frac{\lambda}{1 + e^{- x}} - \frac{\lambda}{2},
$$
since its derivative decreases slower. We have the following formulas for the penalty function $w(r)$:
$$
  w(r) = \sum_{l = 1}^{d} w(r_l) = \sum_{l = 1}^{d} \left( \frac{\lambda}{1 + e^{- r_l}} - \frac{\lambda}{2} \right) \\
  \frac{\mathrm{d} w(r_l)}{\mathrm{d} r_l} =  \frac{\lambda e^{-r_l}}{(1 + e^{-r_l})^2} \\
  \frac{\mathrm{d}^2 w(r_l)}{\mathrm{d} r_l^2} = \frac{-\lambda  e^{-r_l} (1 + e^{-r_l})^2 + 2e^{-r_l}(1 + e^{-r_l})\lambda e^{-r_l}}{(1 + e^{-r_l})^4} \\
  = \frac{-\lambda  e^{-r_l} (1 + e^{-r_l}) + 2e^{-r_l}\lambda e^{-r_l}}{(1 + e^{-r_l})^3} =
  \lambda \frac{- e^{-r_l} + e^{-2r_l}}{(1 + e^{-r_l})^3}
$$
The pseudo-algorithm is:

* Initialize `maxminOrder`, `NNarray`, `theta`
* Compute `grad` w.r.t. all parameters
* For `i` in `1 : niter`
  * Optimize SR without penalty to reach the optimal $\theta_1$ and log-likelihood $l_1$
  * Select `k` predictors based on the `grad`
  * Optimize SR without penalty to reach the optimal $\theta_2$ andlog-likelihood $l_2$, notice that $\|\theta_2\|_0 - \|\theta_1\|_0 = k$
  * $c_{\lambda} = 1.0$
  * While($c_{\lambda}$ has not been tried)
    * Choose $\lambda = 2 c_{\lambda}(l_2 - l_1)$ 
    * Optimize relevance or SR with penalty with two starting values, $\theta_1$ and $\theta_2$, whose result is denoted by $\theta_3$
    * If($\|\theta_3\|_0 - \|\theta_1\|_0 > 1$)
      * $c_{\lambda} = c_{\lambda} + 0.1$
    * Else If($\|\theta_3\|_0 - \|\theta_1\|_0 < 1$)
      * $c_{\lambda} = c_{\lambda} - 0.1$
    * Else
      * break
  * $\theta = \theta_3$
  * Compute LOO-CV score
  * Update `maxminOrderPrev`, `NNarray`, `grad` based on $\theta$

## Experiment
Clean the environment
```{r clean environment}
rm(list = ls())
```
Need to install a modified version of the GpGp R package.
```{r install the moderated GpGp Pkg}
devtools::install_github("https://github.com/SamCao1991/GpGp.git")
library(GpGp)
library(mvtnorm)
library(ggplot2)
library(RColorBrewer)
library(scales)
library(FNN)
source("CQCD.R")
set.seed(123)
```
Simulate GP.
```{r simulate GP, eval = T}
n <- 1e3
d <- 20
m <- 100
k <- 3
rSq <- c(10, 5, 2, 1, 0.5, rep(0, d - 5))^2
sigmasq <- 1.0 # variance
tausq <- 0.05^2 # nugget
locs <- lhs::randomLHS(n, d)
locs <- locs * outer(rep(sqrt(n), n), 
                     1 / sqrt(colSums(locs^2)))
covM <- GpGp::matern25_scaledim_sqrelevance(c(sigmasq, rSq, tausq), locs)
cholM <- t(chol(covM))
y <- as.vector(cholM %*% rnorm(n))
X <- matrix(1, n, 1)
rm(covM, cholM)
```
Function for computing the approximate LOO-CV score, assuming the squared relevance parameterized Matern25 kernel.
```{r LOO-CV score, eval = T}
CV_score <- function(theta)
{
  idxLocsRel <- which(theta[2 : (d + 1)] > 0)
  locsRel <- locs[, idxLocsRel, drop = F]
  thetaRel <- theta[c(1, idxLocsRel + 1, d + 2)]
  dRel <- length(idxLocsRel)
  locsRelScal <- locsRel %*% 
    diag(sqrt(thetaRel[2 : (dRel + 1)]), dRel, dRel)
  NNarray <- get.knnx(locsRelScal, locsRelScal, m + 1)$nn.index
  - vecchia_profbeta_loglik(thetaRel, "matern25_scaledim_sqrelevance",
                            y, X, locsRel, NNarray)$loglik
}
```
Function for fitting with SR without penalty.
```{r func SR regression without penalty, eval = T}
fit_SR <- function(idxNew, niterOut, niterIn)
{
  # remove irrelevant predictors
  thetaRel <- theta[c(1, idxNew, 2 + d)] 
  locsOdrRel <- locsOdr[, idxNew - 1, drop = F] 
  lbRel <- lb[c(1, idxNew, 2 + d)] 
  # opt with SR parameters without penalty
  objfun <- function(theta){
    GpGp::vecchia_profbeta_loglik(theta, "matern25_scaledim_sqrelevance",
                                            yOdr, XOdr, locsOdrRel, 
                                            NNarray)
  }
  objfun_gdfm <- function(theta){
    GpGp::vecchia_profbeta_loglik_grad_info(theta, 
                                            "matern25_scaledim_sqrelevance",
                                            yOdr, XOdr, locsOdrRel, 
                                            NNarray)
  }
  startTimeOpt1 <- Sys.time()
  optObj <- CQCD(objfun, objfun_gdfm, thetaRel, maxIterOut = niterOut, 
                 maxIterIn = niterIn, lb = lbRel, arg_check = arg_check_SR)
  endTimeOpt1 <- Sys.time()
  # cat("Opt with SR without penalty used", 
  #     as.numeric(difftime(endTimeOpt1, startTimeOpt1, units = "secs")), 
  #     "seconds\n")
  optObj
}
```
Logistic penalty functions.
```{r logistic penalty, eval = T}
penfun <- function(theta){
  r <- theta[-c(1, length(theta))]
  rExp <- exp(- r)
  sum(lambda / (1 + rExp) - lambda / 2)
}
dpenfun <- function(theta){
  r <- theta[-c(1, length(theta))]
  rExp <- exp(- r)
  c(0, lambda * rExp / (1 + rExp)^2, 0)
}
ddpenfun <- function(theta){
  r <- theta[-c(1, length(theta))]
  rExp <- exp(- r)
  diag(c(0, lambda * (rExp^2 - rExp) / (1 + rExp)^3, 0))
}
```
Function for fitting with SR with penalty.
```{r func SR regression with penalty, eval = T}
fit_SR_pen <- function(idxNew, niterOut, niterIn)
{
  # remove irrelevant predictors
  thetaRel <- theta[c(1, idxNew, 2 + d)] 
  locsOdrRel <- locsOdr[, idxNew - 1, drop = F] 
  lbRel <- lb[c(1, idxNew, 2 + d)] 
  # opt with SR parameters with penalty
  objfun <- function(theta){
    likObj <- GpGp::vecchia_profbeta_loglik(theta, 
                                            "matern25_scaledim_sqrelevance",
                                            yOdr, XOdr, locsOdrRel, 
                                            NNarray)
    likObj$loglik <- likObj$loglik - penfun(theta)
    likObj
  }
  objfun_gdfm <- function(theta){
    likObj <- GpGp::vecchia_profbeta_loglik_grad_info(theta, 
                                            "matern25_scaledim_sqrelevance",
                                            yOdr, XOdr, locsOdrRel, 
                                            NNarray)
    likObj$loglik <- likObj$loglik - penfun(theta)
    likObj$grad <- likObj$grad - dpenfun(theta)
    likObj$info <- likObj$info + ddpenfun(theta)
    likObj
  }
  startTimeOpt1 <- Sys.time()
  optObj <- CQCD(objfun, objfun_gdfm, thetaRel, maxIterOut = niterOut, 
                 maxIterIn = niterIn, lb = lbRel, arg_check = arg_check_SR)
  endTimeOpt1 <- Sys.time()
  # cat("Opt with SR with penalty used", 
  #     as.numeric(difftime(endTimeOpt1, startTimeOpt1, units = "secs")), 
  #     "seconds\n")
  optObj
}
```
Function for fitting with relevance with penalty.
```{r func relevance regression with penalty, eval = T}
fit_rel_pen <- function(idxNew, niterOut, niterIn)
{
  # remove irrelevant predictors
  thetaRel <- theta[c(1, idxNew, 2 + d)] 
  thetaRel[2 : (1 + length(idxNew))] <- sqrt(thetaRel[2 : (1 + length(idxNew))])
  locsOdrRel <- locsOdr[, idxNew - 1, drop = F] 
  lbRel <- lb[c(1, idxNew, 2 + d)] 
  # opt with SR parameters with penalty
  objfun <- function(theta){
    likObj <- GpGp::vecchia_profbeta_loglik(theta, 
                                            "matern25_scaledim_relevance",
                                            yOdr, XOdr, locsOdrRel, 
                                            NNarray)
    likObj$loglik <- likObj$loglik - penfun(theta)
    likObj
  }
  objfun_gdfm <- function(theta){
    likObj <- GpGp::vecchia_profbeta_loglik_grad_info(theta, 
                                            "matern25_scaledim_relevance",
                                            yOdr, XOdr, locsOdrRel, 
                                            NNarray)
    likObj$loglik <- likObj$loglik - penfun(theta)
    likObj$grad <- likObj$grad - dpenfun(theta)
    likObj$info <- likObj$info + ddpenfun(theta)
    likObj
  }
  startTimeOpt1 <- Sys.time()
  optObj <- CQCD(objfun, objfun_gdfm, thetaRel, maxIterOut = niterOut, 
                 maxIterIn = niterIn, lb = lbRel, arg_check = arg_check_R)
  endTimeOpt1 <- Sys.time()
  # cat("Opt with SR with penalty used", 
  #     as.numeric(difftime(endTimeOpt1, startTimeOpt1, units = "secs")), 
  #     "seconds\n")
  optObj$covparms[2 : (1 + length(idxNew))] <- 
    optObj$covparms[2 : (1 + length(idxNew))]^2
  optObj
}
```
Storage variabels for plotting later.
```{r lambda grid, eval = T}
# define result collecting vars
idxSet <- list()
thetaSet <- list()
scoreSet <- list()
lambdaSet <- list()
```
Initialization.
```{r nitialization, eval = T}
sigmasqInit <- 0.25
tausqInit <- 0.01^2
theta <- c(sigmasqInit, rep(1e-8, d), tausqInit)
lb <- c(0.01^2, rep(0, d), 0.01^2) 
arg_check_SR <- function(x) {sum(sqrt(x[-c(1, length(x))])) > 1e-4}
arg_check_R <- function(x) {sum(x[-c(1, length(x))]) > 1e-4}
idx <- c()
XOdr <- X
yOdr <- y
locsOdr <- locs
NNarray <- find_ordered_nn(locs, m = m)
gradObj <- vecchia_profbeta_loglik_grad_info(theta, 
                                             "matern25_scaledim_sqrelevance",
                                             yOdr, XOdr, locsOdr, NNarray)
```
Loop over lambda
```{r loop over lambda, eval = T}
niter <- 5
startTime <- Sys.time()
for(i in 1 : niter)
{
  cat("\n====================================\n")
  cat("i =", i, "\n")
  # Opt without penalty
  if(length(idx) > 0)
  {
    cat("\nOptimization with idx without penalty\n")
    optObj <- fit_SR(idx, 10, 40)
    loglk <- optObj$loglik
    theta[c(1, idx, d + 2)] <- optObj$covparms
    theta1 <- optObj$covparms
  }else
  {
    loglk <- gradObj$loglik
  }
  # select covariates
  odrDec <- order(gradObj$grad[2 : (d + 1)], decreasing = T)
  if(length(idx) == 0)
    idxNew <- 1 + odrDec[1 : k]
  else
    idxNew <- c(idx, 1 + setdiff(odrDec, idx - 1)[1 : k])
  cat("Selected var:", setdiff(idxNew, idx), "\n")
  # Opt without penalty
  cat("\nOptimization with idxNew without penalty\n")
  optObj <- fit_SR(idxNew, 20, 40)
  loglkNew <- optObj$loglik
  theta2 <- optObj$covparms
  cLambda <- 1.0
  cLambdaPrev <- -1
  while(TRUE)
  {
    # Define lambda
    lambda = 2 * cLambda * (loglkNew - loglk)
    cat("lambda = ", lambda, "\n")
    # Opt with penalty
    cat("\nOptimization from theta2 with penalty\n")
    theta <- rep(0, d + 2)
    theta[c(1, idxNew, d + 2)] <- theta2
    optObj <- fit_SR_pen(idxNew, 20, 40)
    if(length(idx) > 0)
    {
      theta <- rep(0, d + 2)
      theta[c(1, idx, d + 2)] <- theta1
      cat("\nOptimization from theta1 with penalty\n")
      optObjTmp <- fit_SR_pen(idxNew, 20, 40) # try a diff start point
      if(optObjTmp$loglik > optObj$loglik)
        optObj <- optObjTmp
    }
    # Check if the number of new predictors is 1
    idxLocZero <- which(optObj$covparms[2 : (1 + length(idxNew))] == 0)
    if(length(idxLocZero) < k - 1)
      cLambda <- cLambda + 0.1
    else if(length(idxLocZero) > k - 1)
      cLambda <- cLambda - 0.1
    else 
      break
    # Check if infinite loop occurs
    if(cLambda == cLambdaPrev)
        stop("\nInfinite loop in choosing lambda\n")
    cLambdaPrev <- cLambda
  }
  # Take out zero SR
  thetaRel <- optObj$covparms 
  cat("Predictor", idxNew[idxLocZero], "are zerod out \n")
  idxNew <- idxNew[-idxLocZero]
  thetaRel <- thetaRel[-(idxLocZero + 1)]
  # Update theta
  theta <- rep(0, d + 2)
  theta[c(1, idxNew, d + 2)] <- thetaRel
  idx <- idxNew
  # order and compute grad
  locsRel <- locs[, idx - 1, drop = F]
  locsRelScal <- locsRel %*% diag(sqrt(thetaRel[2 : (length(idx) + 1)]), 
                               length(idx), length(idx))
  odr <- order_maxmin(locsRelScal)
  yOdr <- y[odr]
  locsRelScalOdr <- locsRelScal[odr, , drop = F]
  XOdr <- X[odr, , drop = F]
  locsOdr <- locs[odr, , drop = F]
  NNarray <- find_ordered_nn(locsRelScalOdr, m = m)
  gradObj <- vecchia_profbeta_loglik_grad_info(theta, 
                                              "matern25_scaledim_sqrelevance",
                                              yOdr, XOdr, locsOdr, NNarray)
  # Store results
  idxSet[[i]] <- idx
  thetaSet[[i]] <- theta
  scoreSet[[i]] <- CV_score(theta)
  lambdaSet[[i]] <- lambda
}
```

## Verdict




