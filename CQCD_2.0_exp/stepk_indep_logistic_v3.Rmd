---
title: "SR with Logistic Penalty V3.0"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Method 
The penalty from V2.0:
$$
  \frac{\lambda}{1 + e^{- x}} - \frac{\lambda}{2},
$$
seems insufficiently steep. Switching back to the penalty used in V1.0:
$$
  \frac{\lambda}{1 + e^{-\lambda x}} - \frac{\lambda}{2}.
$$

The pseudo-algorithm is approximately the same as V2.0:
* Initialize `maxminOrder`, `NNarray`, `theta`
* Compute `grad` w.r.t. all parameters
* For `i` in `1 : niter`
  * Optimize SR without penalty to reach the optimal log-likelihood $l_1$
  * Select `k` predictors based on the `grad`
  * Optimize SR without penalty to reach the optimal log-likelihood $l_2$
  * Choose $0.5\lambda = 0.9(l_2 - l_1)$ 
  * Optimize relevance or SR with penalty
  * Compute LOO-CV score
  * Update `maxminOrderPrev`, `NNarray`, `grad` based on the new `theta`

## Experiment
Clean the environment
```{r clean environment}
rm(list = ls())
```
Need to install a modified version of the GpGp R package.
```{r install the moderated GpGp Pkg}
devtools::install_github("https://github.com/SamCao1991/GpGp.git")
library(GpGp)
library(mvtnorm)
library(ggplot2)
library(RColorBrewer)
library(scales)
library(FNN)
source("CQCD.R")
set.seed(123)
```
Simulate GP.
```{r simulate GP, eval = F}
n <- 1e3
d <- 20
m <- 100
k <- 3
rSq <- c(10, 5, 2, 1, 0.5, rep(0, d - 5))^2
sigmasq <- 1.0 # variance
tausq <- 0.05^2 # nugget
locs <- lhs::randomLHS(n, d)
locs <- locs * outer(rep(sqrt(n), n), 
                     1 / sqrt(colSums(locs^2)))
covM <- GpGp::matern25_scaledim_sqrelevance(c(sigmasq, rSq, tausq), locs)
cholM <- t(chol(covM))
y <- as.vector(cholM %*% rnorm(n))
X <- matrix(1, n, 1)
rm(covM, cholM)
```
Function for computing the approximate LOO-CV score, assuming the squared relevance parameterized Matern25 kernel.
```{r LOO-CV score, eval = F}
CV_score <- function(theta)
{
  idxLocsRel <- which(theta[2 : (d + 1)] > 0)
  locsRel <- locs[, idxLocsRel, drop = F]
  thetaRel <- theta[c(1, idxLocsRel + 1, d + 2)]
  dRel <- length(idxLocsRel)
  locsRelScal <- locsRel %*% 
    diag(sqrt(thetaRel[2 : (dRel + 1)]), dRel, dRel)
  NNarray <- get.knnx(locsRelScal, locsRelScal, m + 1)$nn.index
  - vecchia_profbeta_loglik(thetaRel, "matern25_scaledim_sqrelevance",
                            y, X, locsRel, NNarray)$loglik
}
```
Function for fitting with SR without penalty.
```{r func SR regression without penalty, eval = F}
fit_SR <- function(idxNew, niterOut, niterIn)
{
  # remove irrelevant predictors
  thetaRel <- theta[c(1, idxNew, 2 + d)] 
  locsOdrRel <- locsOdr[, idxNew - 1, drop = F] 
  lbRel <- lb[c(1, idxNew, 2 + d)] 
  # opt with SR parameters without penalty
  objfun <- function(theta){
    GpGp::vecchia_profbeta_loglik(theta, "matern25_scaledim_sqrelevance",
                                            yOdr, XOdr, locsOdrRel, 
                                            NNarray)
  }
  objfun_gdfm <- function(theta){
    GpGp::vecchia_profbeta_loglik_grad_info(theta, 
                                            "matern25_scaledim_sqrelevance",
                                            yOdr, XOdr, locsOdrRel, 
                                            NNarray)
  }
  startTimeOpt1 <- Sys.time()
  optObj <- CQCD(objfun, objfun_gdfm, thetaRel, maxIterOut = niterOut, 
                 maxIterIn = niterIn, lb = lbRel, arg_check = arg_check_SR)
  endTimeOpt1 <- Sys.time()
  # cat("Opt with SR without penalty used", 
  #     as.numeric(difftime(endTimeOpt1, startTimeOpt1, units = "secs")), 
  #     "seconds\n")
  optObj
}
```
Logistic penalty functions.
```{r logistic penalty, eval = F}
penfun <- function(theta){
  r <- theta[-c(1, length(theta))]
  rExp <- exp(- lambda * r)
  sum(lambda / (1 + rExp) - lambda / 2)
}
dpenfun <- function(theta){
  r <- theta[-c(1, length(theta))]
  rExp <- exp(- lambda * r)
  c(0, lambda^2 * rExp / (1 + rExp)^2, 0)
}
ddpenfun <- function(theta){
  r <- theta[-c(1, length(theta))]
  rExp <- exp(- lambda * r)
  diag(c(0, lambda^3 * (rExp^2 - rExp) / (1 + rExp)^3, 0))
}
```
Function for fitting with SR with penalty.
```{r func SR regression with penalty, eval = F}
fit_SR_pen <- function(idxNew, niterOut, niterIn)
{
  # remove irrelevant predictors
  thetaRel <- theta[c(1, idxNew, 2 + d)] 
  locsOdrRel <- locsOdr[, idxNew - 1, drop = F] 
  lbRel <- lb[c(1, idxNew, 2 + d)] 
  # opt with SR parameters with penalty
  objfun <- function(theta){
    likObj <- GpGp::vecchia_profbeta_loglik(theta, 
                                            "matern25_scaledim_sqrelevance",
                                            yOdr, XOdr, locsOdrRel, 
                                            NNarray)
    likObj$loglik <- likObj$loglik - penfun(theta)
    likObj
  }
  objfun_gdfm <- function(theta){
    likObj <- GpGp::vecchia_profbeta_loglik_grad_info(theta, 
                                            "matern25_scaledim_sqrelevance",
                                            yOdr, XOdr, locsOdrRel, 
                                            NNarray)
    likObj$loglik <- likObj$loglik - penfun(theta)
    likObj$grad <- likObj$grad - dpenfun(theta)
    likObj$info <- likObj$info + ddpenfun(theta)
    likObj
  }
  startTimeOpt1 <- Sys.time()
  optObj <- CQCD(objfun, objfun_gdfm, thetaRel, maxIterOut = niterOut, 
                 maxIterIn = niterIn, lb = lbRel, arg_check = arg_check_SR)
  endTimeOpt1 <- Sys.time()
  # cat("Opt with SR with penalty used", 
  #     as.numeric(difftime(endTimeOpt1, startTimeOpt1, units = "secs")), 
  #     "seconds\n")
  optObj
}
```
Function for fitting with relevance with penalty.
```{r func relevance regression with penalty, eval = F}
fit_rel_pen <- function(idxNew, niterOut, niterIn)
{
  # remove irrelevant predictors
  thetaRel <- theta[c(1, idxNew, 2 + d)] 
  thetaRel[2 : (1 + length(idxNew))] <- sqrt(thetaRel[2 : (1 + length(idxNew))])
  locsOdrRel <- locsOdr[, idxNew - 1, drop = F] 
  lbRel <- lb[c(1, idxNew, 2 + d)] 
  # opt with SR parameters with penalty
  objfun <- function(theta){
    likObj <- GpGp::vecchia_profbeta_loglik(theta, 
                                            "matern25_scaledim_relevance",
                                            yOdr, XOdr, locsOdrRel, 
                                            NNarray)
    likObj$loglik <- likObj$loglik - penfun(theta)
    likObj
  }
  objfun_gdfm <- function(theta){
    likObj <- GpGp::vecchia_profbeta_loglik_grad_info(theta, 
                                            "matern25_scaledim_relevance",
                                            yOdr, XOdr, locsOdrRel, 
                                            NNarray)
    likObj$loglik <- likObj$loglik - penfun(theta)
    likObj$grad <- likObj$grad - dpenfun(theta)
    likObj$info <- likObj$info + ddpenfun(theta)
    likObj
  }
  startTimeOpt1 <- Sys.time()
  optObj <- CQCD(objfun, objfun_gdfm, thetaRel, maxIterOut = niterOut, 
                 maxIterIn = niterIn, lb = lbRel, arg_check = arg_check_R)
  endTimeOpt1 <- Sys.time()
  # cat("Opt with SR with penalty used", 
  #     as.numeric(difftime(endTimeOpt1, startTimeOpt1, units = "secs")), 
  #     "seconds\n")
  optObj$covparms[2 : (1 + length(idxNew))] <- 
    optObj$covparms[2 : (1 + length(idxNew))]^2
  optObj
}
```
Storage variabels for plotting later.
```{r lambda grid, eval = F}
# define result collecting vars
idxSet <- list()
thetaSet <- list()
scoreSet <- list()
lambdaSet <- list()
```
Initialization.
```{r nitialization, eval = F}
sigmasqInit <- 0.25
tausqInit <- 0.01^2
theta <- c(sigmasqInit, rep(1e-8, d), tausqInit)
lb <- c(0.01^2, rep(0, d), 0.01^2) 
arg_check_SR <- function(x) {sum(sqrt(x[-c(1, length(x))])) > 1e-4}
arg_check_R <- function(x) {sum(x[-c(1, length(x))]) > 1e-4}
idx <- c()
XOdr <- X
yOdr <- y
locsOdr <- locs
NNarray <- find_ordered_nn(locs, m = m)
gradObj <- vecchia_profbeta_loglik_grad_info(theta, 
                                             "matern25_scaledim_sqrelevance",
                                             yOdr, XOdr, locsOdr, NNarray)
```
Loop over lambda
```{r loop over lambda, eval = F}
niter <- 5
startTime <- Sys.time()
for(i in 1 : niter)
{
  cat("\n====================================\n")
  cat("i =", i, "\n")
  # Opt without penalty
  if(length(idx) > 0)
  {
    cat("\nOptimization with idx without penalty\n")
    optObj <- fit_SR(idx, 10, 40)
    loglk <- optObj$loglik
    theta[c(1, idx, d + 2)] <- optObj$covparms
    thetaTmp <- optObj$covparms
  }else
    loglk <- gradObj$loglik
  # select covariates
  odrDec <- order(gradObj$grad[2 : (d + 1)], decreasing = T)
  if(length(idx) == 0)
    idxNew <- 1 + odrDec[1 : k]
  else
    idxNew <- c(idx, 1 + setdiff(odrDec, idx - 1)[1 : k])
  cat("Selected var:", setdiff(idxNew, idx), "\n")
  # Opt without penalty
  cat("\nOptimization with idxNew without penalty\n")
  optObj <- fit_SR(idxNew, 20, 40)
  loglkNew <- optObj$loglik
  theta[c(1, idxNew, d + 2)] <- optObj$covparms
  # Define lambda
  lambda = 2 * 0.9 * (loglkNew - loglk)
  cat("lambda = ", lambda, "\n")
  # Opt with penalty
  cat("\nOptimization with idxNew with penalty\n")
  optObj <- fit_SR_pen(idxNew, 20, 40)
  if(length(idx) > 0)
  {
    cat("\nSet idx", setdiff(idxNew, idx), "to zero\n")
    theta[setdiff(idxNew, idx)] <- 0
    theta[c(1, idx, d + 2)] <- thetaTmp
    cat("\nOptimization with idxNew with penalty\n")
    optObjTmp <- fit_SR_pen(idxNew, 20, 40) # try a diff start point
    if(optObjTmp$loglik > optObj$loglik)
      optObj <- optObjTmp
  }
  # Take out zero SR
  thetaRel <- optObj$covparms 
  idxLocZero <- which(thetaRel[2 : (1 + length(idxNew))] == 0)
  if(length(idxLocZero) > 0)
  {
    cat("Predictor", idxNew[idxLocZero], "are zerod out \n")
    idxNew <- idxNew[-idxLocZero]
    thetaRel <- thetaRel[-(idxLocZero + 1)]
  }
  theta <- rep(0, d + 2)
  theta[c(1, idxNew, d + 2)] <- thetaRel
  idx <- idxNew
  # order and compute grad
  locsRel <- locs[, idx - 1, drop = F]
  locsRelScal <- locsRel %*% diag(sqrt(thetaRel[2 : (length(idx) + 1)]), 
                               length(idx), length(idx))
  odr <- order_maxmin(locsRelScal)
  yOdr <- y[odr]
  locsRelScalOdr <- locsRelScal[odr, , drop = F]
  XOdr <- X[odr, , drop = F]
  locsOdr <- locs[odr, , drop = F]
  NNarray <- find_ordered_nn(locsRelScalOdr, m = m)
  gradObj <- vecchia_profbeta_loglik_grad_info(theta, 
                                              "matern25_scaledim_sqrelevance",
                                              yOdr, XOdr, locsOdr, NNarray)
  # Store results
  idxSet[[i]] <- idx
  thetaSet[[i]] <- theta
  scoreSet[[i]] <- CV_score(theta)
  lambdaSet[[i]] <- lambda
}
```

## Verdict




