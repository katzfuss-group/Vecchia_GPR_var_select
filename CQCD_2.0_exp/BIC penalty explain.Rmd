---
title: "BIC_penalty"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Why a penalty is needed
To see how to converge the irrelevant predictors' parameters to zero, first we tried to decide whether it is the optimization algorithm's problem or the objective function's problem. So far it seems more like the objective function's problem. Specifically, I set the irrelevant predictors' parameters to zero while keeping the other parameters unchanged, the objective function increases (we want smaller objective function values). However, if we take BIC penalty $\frac{\log n}{2} |d_{+}|$ into consideration, the objective function distinctively decreases.

Therefore, the objective function probably need to be complemented by a penalty function. However, BIC cannot be directly used as a penalty because it's a step function.

## Why BIC penalty

One advantage we sell our method for is avoiding selecting penalty parameters. So we want our method to be independent from penalty parameters or at least, the parameters can be chosen easily. At Line~9 of our Algo 5, $r_{{BIC}}(\cdot)$ is used. I just tried a simple design:

* $\hat{r}$ is the output from $h_{sr}(\cdot)$
* $r_{{BIC}}(\cdot)$ is a truncated linear function:
```{r BIC penalty, echo=FALSE}
library(ggplot2)
mydf <- matrix(NA, 1e3, 2)
mydf[, 1] <- seq(-1, 2, length.out = 1e3)
mydf[, 2] <- mydf[, 1] * (mydf[, 1] < 1 & mydf[, 1] > 0)
mydf[, 2][mydf[, 1] >= 1] <- 1
colnames(mydf) <- c("x", "y")
mydf <- as.data.frame(mydf)

ggplot(mydf, aes(x = x, y = y)) + geom_point() +
  scale_x_continuous(name = expression(r[l]), 
                   breaks = c(0, 0.5, 1),
                   labels = c(expression(0), expression(hat(r)[l]), 
                              expression(2 * hat(r)[l]))) +
  scale_y_continuous(name = "penalty", 
                   breaks = c(0, 0.5, 1),
                   labels = c(expression(0), expression(logn / 4), expression(logn / 2)))
```

After adding in this penalty function, the results appear pretty good! The previous CQCD optimization without penalty converged to 

```
0.9703 5.0106 2.0334 10.1705 0.9772 0.5076 0.0131 0.0126 0.0134 0.0124 0.0134 0.0025 
```

Keeping running CQCD with the BIC penalty gives the following results:

```
Iter 0: 
pars =  0.9703 5.0106 2.0334 10.1705 0.9772 0.5076 0.0131 0.0126 0.0134 0.0124 0.0134 0.0025   
obj = -8668.004786  

Lower bounded coordinate descent is used 
Iter 1: 
pars =  1.0308 4.9302 2.0001 10.0079 0.9604 0.4977 0.0066 0.0056 0.0073 0.005 0.0071 0.0024   
obj = -8672.037117  

Lower bounded coordinate descent is used 
Iter 2: 
pars =  1.033 4.9336 2.0014 10.0139 0.961 0.4975 0 0 0.0013 0 3e-04 0.0025   
obj = -8675.65748  

Lower bounded coordinate descent is used 
Iter 3: 
pars =  1.013 4.9579 2.0114 10.0635 0.9659 0.5003 0 0 0 0 0 0.0025   
obj = -8675.963846  

Lower bounded coordinate descent is used 
Iter 4: 
pars =  1.0048 4.9676 2.0155 10.0835 0.9679 0.5015 0 0 0 0 0 0.0025   
obj = -8675.970825  

Lower bounded coordinate descent is used 
Iter 5: 
pars =  1.0013 4.9719 2.0172 10.0922 0.9687 0.5021 0 0 0 0 0 0.0025   
obj = -8675.972135  

Lower bounded coordinate descent is used 
Iter 6: 
pars =  1.0002 4.9731 2.0177 10.0948 0.9689 0.5022 0 0 0 0 0 0.0025   
obj = -8675.972324
```

## Some concerns
So far it seems to be working. In terms of using it in our paper, not sure about

* this penalty seems arbitrary or controversial (because it's not smoothed)
* this would be in conflict with not using a penalty function. If so, we can change the words to no requirement for choosing any penalty parameter






